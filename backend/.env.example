# RAG with Graph Store - Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# SECURITY (REQUIRED)
# =============================================================================
# Generate with: openssl rand -hex 32
SECRET_KEY=your-secret-key-here-generate-with-openssl-rand-hex-32

# =============================================================================
# REDIS
# =============================================================================
# Used for token blocklist and caching
REDIS_URL=redis://localhost:6379/0

# =============================================================================
# POSTGRESQL (for LangGraph checkpointing)
# =============================================================================
# Docker default: postgresql://postgres:postgres@localhost:5432/ragapp
POSTGRES_URI=postgresql://postgres:password@localhost:5432/ragapp

# =============================================================================
# NEO4J AURA (REQUIRED)
# =============================================================================
# Get from: https://console.neo4j.io
# URI format: neo4j+s://xxxxx.databases.neo4j.io
NEO4J_URI=neo4j+s://xxxxx.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-aura-password
NEO4J_DATABASE=neo4j

# =============================================================================
# QDRANT (REQUIRED)
# =============================================================================
# Local Docker: localhost
# Qdrant Cloud: your-cluster-url.qdrant.io
QDRANT_HOST=localhost
QDRANT_PORT=6333
# Only for Qdrant Cloud (leave empty for local)
QDRANT_API_KEY=
QDRANT_COLLECTION=documents

# =============================================================================
# LLM PROVIDER SELECTION
# =============================================================================
# Choose your LLM provider: "openai", "ollama", or "anthropic"
# For OpenAI-compatible APIs (Groq, DeepSeek, Azure), use "openai" + OPENAI_BASE_URL
LLM_PROVIDER=openai
# Choose your embedding provider: "openai" or "ollama"
# (Anthropic does not offer embeddings â€” use openai or ollama)
EMBEDDING_PROVIDER=openai

# =============================================================================
# OPENAI (required when LLM_PROVIDER=openai or EMBEDDING_PROVIDER=openai)
# =============================================================================
# Get from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4o
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# CRITICAL: Must match embedding model output dimensions
# text-embedding-3-small = 1536, text-embedding-3-large = 3072
OPENAI_EMBEDDING_DIMENSIONS=1536
# Optional: custom base URL (for Azure OpenAI, proxies, etc.)
# OPENAI_BASE_URL=

# =============================================================================
# ANTHROPIC (required when LLM_PROVIDER=anthropic)
# =============================================================================
# Get from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# =============================================================================
# OLLAMA (required when LLM_PROVIDER=ollama or EMBEDDING_PROVIDER=ollama)
# =============================================================================
# Install Ollama: https://ollama.ai
# Then pull models: ollama pull llama3.2 && ollama pull nomic-embed-text
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# nomic-embed-text = 768, mxbai-embed-large = 1024
OLLAMA_EMBEDDING_DIMENSIONS=768

# =============================================================================
# DOCUMENT PROCESSING
# =============================================================================
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
MAX_UPLOAD_SIZE_MB=50

# =============================================================================
# LOGGING
# =============================================================================
LOG_LEVEL=INFO
