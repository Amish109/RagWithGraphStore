"""LLM generation service for document Q&A with strict context constraints.

This module provides answer generation using LangChain with configurable LLM provider.
Implements strict "I don't know" fallback to prevent hallucination.
Supports both synchronous and streaming responses.
"""

from typing import AsyncGenerator, Dict, List

from langchain_core.prompts import ChatPromptTemplate

from app.services.llm_provider import get_llm


# Initialize LLM with deterministic settings
llm = get_llm(temperature=0)


async def generate_answer(query: str, context: List[Dict]) -> str:
    """Generate answer using LLM with strict context-only constraint.

    CRITICAL: Prevents hallucination by enforcing "I don't know" fallback.
    Addresses QRY-04 requirement.

    Args:
        query: User's question.
        context: List of context chunks with 'text' and optional 'filename' keys.

    Returns:
        Generated answer string.
    """
    # Assemble context from chunks, including entity relationships when present
    context_parts = []
    for chunk in context:
        part = f"[Source: {chunk.get('filename', 'Unknown')}]\n{chunk['text']}"
        entity_rels = chunk.get("entity_relations", [])
        if entity_rels:
            rel_lines = [
                f"  - {r['entity']} --[{r.get('relation', 'RELATED_TO')}]--> {r['related_entity']}"
                for r in entity_rels[:5]
                if r.get("entity") and r.get("related_entity")
            ]
            if rel_lines:
                part += "\n\nRelated entities:\n" + "\n".join(rel_lines)
        context_parts.append(part)
    context_text = "\n\n".join(context_parts)

    # Prompt template with strict constraints
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful Q&A assistant. Answer questions ONLY based on the provided context, which may include documents, shared memory facts, and entity relationships from a knowledge graph.

CRITICAL INSTRUCTIONS:
- If the context does not contain information to answer the question, respond EXACTLY with: "I don't know. I couldn't find information about this in the provided context."
- Do not use any knowledge outside the provided context
- Cite the source (document name or "Shared Memory") when referencing information
- Use entity relationships to provide more comprehensive, cross-document answers when relevant
- Be concise and direct"""),
        ("user", """Context:
{context}

Question: {query}

Answer:""")
    ])

    # Generate response
    messages = prompt.format_messages(context=context_text, query=query)
    response = await llm.ainvoke(messages)

    return response.content


async def generate_answer_no_context() -> str:
    """Return standard "I don't know" response when no context available.

    Returns:
        Standard fallback message.
    """
    return "I don't know. I couldn't find any relevant information in your documents."


async def stream_answer(query: str, context: List[Dict]) -> AsyncGenerator[str, None]:
    """Stream LLM response token by token.

    Uses LangChain ChatOpenAI.astream() for async token streaming.
    CRITICAL: Creates streaming LLM instance with streaming=True for token-by-token output.

    Args:
        query: User's question.
        context: List of context chunks with 'text' and optional 'filename' keys.

    Yields:
        String chunks as they are generated by the LLM.
    """
    # Assemble context from chunks, including entity relationships when present
    context_parts = []
    for chunk in context:
        part = f"[Source: {chunk.get('filename', 'Unknown')}]\n{chunk['text']}"
        entity_rels = chunk.get("entity_relations", [])
        if entity_rels:
            rel_lines = [
                f"  - {r['entity']} --[{r.get('relation', 'RELATED_TO')}]--> {r['related_entity']}"
                for r in entity_rels[:5]
                if r.get("entity") and r.get("related_entity")
            ]
            if rel_lines:
                part += "\n\nRelated entities:\n" + "\n".join(rel_lines)
        context_parts.append(part)
    context_text = "\n\n".join(context_parts)

    # Same prompt template as non-streaming
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful Q&A assistant. Answer questions ONLY based on the provided context, which may include documents, shared memory facts, and entity relationships from a knowledge graph.

CRITICAL INSTRUCTIONS:
- If the context does not contain information to answer the question, respond EXACTLY with: "I don't know. I couldn't find information about this in the provided context."
- Do not use any knowledge outside the provided context
- Cite the source (document name or "Shared Memory") when referencing information
- Use entity relationships to provide more comprehensive, cross-document answers when relevant
- Be concise and direct"""),
        ("user", """Context:
{context}

Question: {query}

Answer:""")
    ])

    messages = prompt.format_messages(context=context_text, query=query)

    # Create streaming LLM instance
    streaming_llm = get_llm(temperature=0, streaming=True)

    # Stream tokens using astream
    async for chunk in streaming_llm.astream(messages):
        if chunk.content:
            yield chunk.content
