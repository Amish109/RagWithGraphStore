"""LLM generation service for document Q&A with strict context constraints.

This module provides answer generation using LangChain with configurable LLM provider.
Implements strict "I don't know" fallback to prevent hallucination.
Supports both synchronous and streaming responses.
"""

from typing import AsyncGenerator, Dict, List

from langchain_core.prompts import ChatPromptTemplate

from app.services.llm_provider import get_llm


# Initialize LLM with deterministic settings
llm = get_llm(temperature=0)


async def generate_answer(query: str, context: List[Dict]) -> str:
    """Generate answer using LLM with strict context-only constraint.

    CRITICAL: Prevents hallucination by enforcing "I don't know" fallback.
    Addresses QRY-04 requirement.

    Args:
        query: User's question.
        context: List of context chunks with 'text' and optional 'filename' keys.

    Returns:
        Generated answer string.
    """
    # Assemble context from chunks
    context_text = "\n\n".join([
        f"[Source: {chunk.get('filename', 'Unknown')}]\n{chunk['text']}"
        for chunk in context
    ])

    # Prompt template with strict constraints
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful Q&A assistant. Answer questions ONLY based on the provided context, which may include documents and shared memory facts.

CRITICAL INSTRUCTIONS:
- If the context does not contain information to answer the question, respond EXACTLY with: "I don't know. I couldn't find information about this in the provided context."
- Do not use any knowledge outside the provided context
- Cite the source (document name or "Shared Memory") when referencing information
- Be concise and direct"""),
        ("user", """Context:
{context}

Question: {query}

Answer:""")
    ])

    # Generate response
    messages = prompt.format_messages(context=context_text, query=query)
    response = await llm.ainvoke(messages)

    return response.content


async def generate_answer_no_context() -> str:
    """Return standard "I don't know" response when no context available.

    Returns:
        Standard fallback message.
    """
    return "I don't know. I couldn't find any relevant information in your documents."


async def stream_answer(query: str, context: List[Dict]) -> AsyncGenerator[str, None]:
    """Stream LLM response token by token.

    Uses LangChain ChatOpenAI.astream() for async token streaming.
    CRITICAL: Creates streaming LLM instance with streaming=True for token-by-token output.

    Args:
        query: User's question.
        context: List of context chunks with 'text' and optional 'filename' keys.

    Yields:
        String chunks as they are generated by the LLM.
    """
    # Assemble context from chunks
    context_text = "\n\n".join([
        f"[Source: {chunk.get('filename', 'Unknown')}]\n{chunk['text']}"
        for chunk in context
    ])

    # Same prompt template as non-streaming
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful Q&A assistant. Answer questions ONLY based on the provided context, which may include documents and shared memory facts.

CRITICAL INSTRUCTIONS:
- If the context does not contain information to answer the question, respond EXACTLY with: "I don't know. I couldn't find information about this in the provided context."
- Do not use any knowledge outside the provided context
- Cite the source (document name or "Shared Memory") when referencing information
- Be concise and direct"""),
        ("user", """Context:
{context}

Question: {query}

Answer:""")
    ])

    messages = prompt.format_messages(context=context_text, query=query)

    # Create streaming LLM instance
    streaming_llm = get_llm(temperature=0, streaming=True)

    # Stream tokens using astream
    async for chunk in streaming_llm.astream(messages):
        if chunk.content:
            yield chunk.content
