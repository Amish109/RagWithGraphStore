"""Celery tasks for document processing and summarization.

Two queues:
- 'celery' (default): Document upload processing
- 'summaries': Background summary pre-generation

Start workers:
  Upload:    celery -A app.celery_app:celery worker --loglevel=info --pool=solo -Q celery
  Summaries: celery -A app.celery_app:celery worker --loglevel=info --pool=solo -Q summaries
"""

import asyncio
import logging

from app.celery_app import celery

logger = logging.getLogger(__name__)


@celery.task(bind=True, name="process_document")
def process_document_task(
    self,
    file_path: str,
    document_id: str,
    user_id: str,
    filename: str,
    file_size: int = 0,
):
    """Celery task wrapping the async document processing pipeline.

    This runs in the Celery worker process. Since the pipeline is async,
    we create a fresh event loop per task to avoid "Event loop is closed"
    errors from asyncio.run() closing the loop and leaving cached HTTP
    clients (Ollama/LangChain) with stale loop references.
    """
    from app.services.document_processor import process_document_pipeline

    logger.info(f"Celery task started: {filename} (id: {document_id})")

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(
            process_document_pipeline(
                file_path=file_path,
                document_id=document_id,
                user_id=user_id,
                filename=filename,
                file_size=file_size,
            )
        )
        logger.info(f"Celery task completed: {filename} (id: {document_id})")
    except Exception as e:
        logger.error(f"Celery task failed: {filename} (id: {document_id}): {e}")
        # task_tracker.fail() is already called in process_document_pipeline
        raise
    finally:
        try:
            # Cancel any remaining tasks and shut down gracefully
            pending = asyncio.all_tasks(loop)
            for task in pending:
                task.cancel()
            if pending:
                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            loop.close()


@celery.task(bind=True, name="generate_single_summary")
def generate_single_summary_task(
    self,
    document_id: str,
    user_id: str,
    summary_type: str,
):
    """Generate a single summary type with streaming to Redis.

    Writes tokens to Redis as they are generated so the frontend can
    poll and display partial results in real-time. When complete, the
    full text is stored as a completed summary.

    This task is independent of the frontend connection â€” it keeps
    running even if the user refreshes or switches tabs.
    """
    from app.services.summary_storage import (
        append_stream_chunk,
        clear_stream,
        get_summary,
        get_type_status,
        set_type_status,
        store_summary,
    )
    from app.services.summarization_service import stream_summarize_document

    # Skip if already completed
    if get_summary(document_id, summary_type):
        logger.info(f"Summary already cached, skipping: {document_id}:{summary_type}")
        return

    # Skip if already being generated by another task
    status = get_type_status(document_id, summary_type)
    if status == "generating":
        logger.info(f"Summary already generating, skipping: {document_id}:{summary_type}")
        return

    logger.info(f"Streaming summary generation: {document_id}:{summary_type}")
    set_type_status(document_id, summary_type, "generating")
    clear_stream(document_id, summary_type)

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        async def _generate():
            full_text = []
            async for item in stream_summarize_document(
                document_id=document_id,
                user_id=user_id,
                summary_type=summary_type,
            ):
                if item["type"] == "token":
                    chunk = item["content"]
                    full_text.append(chunk)
                    append_stream_chunk(document_id, summary_type, chunk)

            # Store completed summary and clean up stream
            if full_text:
                final_text = "".join(full_text)
                store_summary(document_id, summary_type, final_text)
                set_type_status(document_id, summary_type, "completed")
                clear_stream(document_id, summary_type)
                logger.info(f"Cached {summary_type} summary for {document_id}: {len(final_text)} chars")

        loop.run_until_complete(_generate())

    except Exception as e:
        logger.error(f"Summary generation failed: {document_id}:{summary_type}: {e}")
        set_type_status(document_id, summary_type, "failed")
    finally:
        try:
            pending = asyncio.all_tasks(loop)
            for task in pending:
                task.cancel()
            if pending:
                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            loop.close()


@celery.task(bind=True, name="generate_summaries")
def generate_summaries_task(
    self,
    document_id: str,
    user_id: str,
    filename: str,
):
    """Pre-generate all 4 summary types after document upload.

    Dispatches individual generate_single_summary tasks for each type.
    This way each type streams to Redis independently.
    """
    from app.services.summary_storage import SUMMARY_TYPES, get_summary

    logger.info(f"Dispatching summary generation for: {filename} (id: {document_id})")

    for summary_type in SUMMARY_TYPES:
        if get_summary(document_id, summary_type):
            logger.info(f"Summary already cached, skipping: {document_id}:{summary_type}")
            continue

        generate_single_summary_task.apply_async(
            kwargs={
                "document_id": document_id,
                "user_id": user_id,
                "summary_type": summary_type,
            },
            queue="summaries",
        )
        logger.info(f"Dispatched {summary_type} summary task for {filename}")
