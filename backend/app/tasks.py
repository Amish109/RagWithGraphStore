"""Celery tasks for document processing, summarization, and entity extraction.

Three queues:
- 'celery' (default): Document upload processing
- 'summaries': Background summary pre-generation
- 'entities': Background entity extraction (GraphRAG)

Start workers (all queues):
  celery -A app.celery_app:celery worker --loglevel=info --pool=solo -Q celery,summaries,entities
"""

import asyncio
import json
import logging

import redis

from app.celery_app import celery
from app.config import settings

logger = logging.getLogger(__name__)


@celery.task(bind=True, name="process_document")
def process_document_task(
    self,
    file_path: str,
    document_id: str,
    user_id: str,
    filename: str,
    file_size: int = 0,
):
    """Celery task wrapping the async document processing pipeline.

    This runs in the Celery worker process. Since the pipeline is async,
    we create a fresh event loop per task to avoid "Event loop is closed"
    errors from asyncio.run() closing the loop and leaving cached HTTP
    clients (Ollama/LangChain) with stale loop references.
    """
    from app.services.document_processor import process_document_pipeline

    logger.info(f"Celery task started: {filename} (id: {document_id})")

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(
            process_document_pipeline(
                file_path=file_path,
                document_id=document_id,
                user_id=user_id,
                filename=filename,
                file_size=file_size,
            )
        )
        logger.info(f"Celery task completed: {filename} (id: {document_id})")
    except Exception as e:
        logger.error(f"Celery task failed: {filename} (id: {document_id}): {e}")
        # task_tracker.fail() is already called in process_document_pipeline
        raise
    finally:
        try:
            # Cancel any remaining tasks and shut down gracefully
            pending = asyncio.all_tasks(loop)
            for task in pending:
                task.cancel()
            if pending:
                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            loop.close()


@celery.task(bind=True, name="generate_single_summary")
def generate_single_summary_task(
    self,
    document_id: str,
    user_id: str,
    summary_type: str,
):
    """Generate a single summary type with streaming to Redis.

    Writes tokens to Redis as they are generated so the frontend can
    poll and display partial results in real-time. When complete, the
    full text is stored as a completed summary.

    This task is independent of the frontend connection — it keeps
    running even if the user refreshes or switches tabs.
    """
    from app.services.summary_storage import (
        append_stream_chunk,
        clear_stream,
        get_summary,
        get_type_status,
        set_type_status,
        store_summary,
    )
    from app.services.summarization_service import stream_summarize_document

    # Skip if already completed
    if get_summary(document_id, summary_type):
        logger.info(f"Summary already cached, skipping: {document_id}:{summary_type}")
        return

    # Skip if already being generated by another task
    status = get_type_status(document_id, summary_type)
    if status == "generating":
        logger.info(f"Summary already generating, skipping: {document_id}:{summary_type}")
        return

    logger.info(f"Streaming summary generation: {document_id}:{summary_type}")
    set_type_status(document_id, summary_type, "generating")
    clear_stream(document_id, summary_type)

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        async def _generate():
            full_text = []
            async for item in stream_summarize_document(
                document_id=document_id,
                user_id=user_id,
                summary_type=summary_type,
            ):
                if item["type"] == "token":
                    chunk = item["content"]
                    full_text.append(chunk)
                    append_stream_chunk(document_id, summary_type, chunk)

            # Store completed summary and clean up stream
            if full_text:
                final_text = "".join(full_text)
                store_summary(document_id, summary_type, final_text)
                set_type_status(document_id, summary_type, "completed")
                clear_stream(document_id, summary_type)
                logger.info(f"Cached {summary_type} summary for {document_id}: {len(final_text)} chars")

        loop.run_until_complete(_generate())

    except Exception as e:
        logger.error(f"Summary generation failed: {document_id}:{summary_type}: {e}")
        set_type_status(document_id, summary_type, "failed")
    finally:
        try:
            pending = asyncio.all_tasks(loop)
            for task in pending:
                task.cancel()
            if pending:
                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            loop.close()


@celery.task(bind=True, name="generate_summaries")
def generate_summaries_task(
    self,
    document_id: str,
    user_id: str,
    filename: str,
):
    """Pre-generate all 4 summary types after document upload.

    Dispatches individual generate_single_summary tasks for each type.
    This way each type streams to Redis independently.
    """
    from app.services.summary_storage import SUMMARY_TYPES, get_summary

    logger.info(f"Dispatching summary generation for: {filename} (id: {document_id})")

    for summary_type in SUMMARY_TYPES:
        if get_summary(document_id, summary_type):
            logger.info(f"Summary already cached, skipping: {document_id}:{summary_type}")
            continue

        generate_single_summary_task.apply_async(
            kwargs={
                "document_id": document_id,
                "user_id": user_id,
                "summary_type": summary_type,
            },
            queue="summaries",
        )
        logger.info(f"Dispatched {summary_type} summary task for {filename}")


# --- Entity extraction Redis helpers ---

_ENTITY_STATUS_PREFIX = "entity_status:"
_ENTITY_STATUS_TTL = 86400  # 24 hours


def _set_entity_status(document_id: str, data: dict) -> None:
    """Store entity extraction status in Redis."""
    r = redis.from_url(settings.REDIS_URL, decode_responses=True)
    r.setex(f"{_ENTITY_STATUS_PREFIX}{document_id}", _ENTITY_STATUS_TTL, json.dumps(data))


def get_entity_status(document_id: str) -> dict | None:
    """Get entity extraction status from Redis."""
    r = redis.from_url(settings.REDIS_URL, decode_responses=True)
    raw = r.get(f"{_ENTITY_STATUS_PREFIX}{document_id}")
    return json.loads(raw) if raw else None


@celery.task(bind=True, name="extract_entities")
def extract_entities_task(
    self,
    document_id: str,
    user_id: str,
    filename: str,
):
    """Extract entities from document chunks in the background.

    Loads chunks from Neo4j, runs LLM-based entity extraction,
    stores results back to Neo4j. Tracks progress in Redis.
    """
    from app.db.neo4j_client import neo4j_driver
    from app.services.entity_extraction_service import extract_entities_batch
    from app.services.indexing_service import store_entities_in_neo4j

    logger.info(f"Entity extraction started: {filename} (id: {document_id})")

    # Load chunks from Neo4j
    with neo4j_driver.session(database=settings.NEO4J_DATABASE) as session:
        result = session.run(
            """
            MATCH (d:Document {id: $doc_id})-[:CONTAINS]->(c:Chunk)
            RETURN c.id AS id, c.text AS text, c.position AS position
            ORDER BY c.position
            """,
            doc_id=document_id,
        )
        chunks = [{"id": r["id"], "text": r["text"], "position": r["position"]} for r in result]

    if not chunks:
        logger.warning(f"No chunks found for entity extraction: {document_id}")
        _set_entity_status(document_id, {
            "status": "failed", "message": "No chunks found",
            "progress": 0, "total_chunks": 0, "completed_chunks": 0, "total_entities": 0,
        })
        return

    total = len(chunks)
    _set_entity_status(document_id, {
        "status": "extracting", "message": f"Extracting entities: 0/{total} chunks",
        "progress": 0, "total_chunks": total, "completed_chunks": 0, "total_entities": 0,
    })

    def progress_callback(completed: int, total_chunks: int, entities_so_far: int):
        pct = int((completed / total_chunks) * 100)
        _set_entity_status(document_id, {
            "status": "extracting",
            "message": f"Extracting entities: {completed}/{total_chunks} chunks",
            "progress": pct,
            "total_chunks": total_chunks,
            "completed_chunks": completed,
            "total_entities": entities_so_far,
        })

    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        entity_results = loop.run_until_complete(
            extract_entities_batch(
                chunks, document_id=document_id,
                progress_callback=progress_callback,
            )
        )

        # Store in Neo4j
        total_entities = 0
        total_rels = 0
        for chunk, entity_result in zip(chunks, entity_results):
            if entity_result.get("entities") or entity_result.get("relationships"):
                store_entities_in_neo4j(
                    chunk_id=chunk["id"],
                    entities=entity_result.get("entities", []),
                    relationships=entity_result.get("relationships", []),
                )
                total_entities += len(entity_result.get("entities", []))
                total_rels += len(entity_result.get("relationships", []))

        _set_entity_status(document_id, {
            "status": "completed",
            "message": f"{total_entities} entities, {total_rels} relationships extracted",
            "progress": 100,
            "total_chunks": total,
            "completed_chunks": total,
            "total_entities": total_entities,
        })
        logger.info(f"Entity extraction completed: {filename} — {total_entities} entities, {total_rels} rels")

    except Exception as e:
        logger.error(f"Entity extraction failed: {filename} (id: {document_id}): {e}")
        _set_entity_status(document_id, {
            "status": "failed", "message": str(e),
            "progress": 0, "total_chunks": total, "completed_chunks": 0, "total_entities": 0,
        })
    finally:
        try:
            pending = asyncio.all_tasks(loop)
            for task in pending:
                task.cancel()
            if pending:
                loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
            loop.run_until_complete(loop.shutdown_asyncgens())
        finally:
            loop.close()
