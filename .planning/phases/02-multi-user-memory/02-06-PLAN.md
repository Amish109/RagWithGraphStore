---
phase: 02-multi-user-memory
plan: 06
type: execute
wave: 3
depends_on: ["02-03", "02-04"]
files_modified:
  - backend/app/config.py
  - backend/app/jobs/cleanup.py
  - backend/app/services/memory_service.py
  - backend/app/api/admin.py
  - backend/app/main.py
  - backend/pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Anonymous data older than TTL is automatically deleted"
    - "Cleanup runs on configured schedule (daily)"
    - "Admin can add facts to shared company-wide memory"
    - "All authenticated users can query shared memory"
    - "Anonymous users cannot access shared memory"
  artifacts:
    - path: "backend/app/jobs/cleanup.py"
      provides: "Scheduled TTL cleanup for anonymous data"
      exports: ["cleanup_expired_anonymous_data", "setup_cleanup_scheduler"]
    - path: "backend/app/api/admin.py"
      provides: "Admin-only endpoints for shared memory"
      exports: ["router"]
  key_links:
    - from: "backend/app/jobs/cleanup.py"
      to: "backend/app/db/neo4j_client.py"
      via: "Delete expired documents and chunks"
      pattern: "neo4j_driver"
    - from: "backend/app/api/admin.py"
      to: "backend/app/core/rbac.py"
      via: "require_admin dependency"
      pattern: "Depends\\(require_admin\\)"
---

<objective>
Implement automatic TTL cleanup for expired anonymous data and admin-only shared memory management. Anonymous data expires after configured period, admin users can add facts to company-wide knowledge base.

Purpose: Prevent unbounded growth of anonymous user data. Enable company-wide knowledge that all authenticated users can query but only admins can modify.

Output: Cleanup scheduler, admin API endpoints for shared memory.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-multi-user-memory/02-RESEARCH.md (Pattern 5: Automatic Data Expiration, Pattern 6: RBAC)
@.planning/phases/02-multi-user-memory/02-03-PLAN.md (RBAC with require_admin)
@.planning/phases/02-multi-user-memory/02-04-PLAN.md (memory service)
@backend/app/db/neo4j_client.py
@backend/app/db/qdrant_client.py
@backend/app/db/mem0_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create scheduled cleanup job for expired anonymous data</name>
  <files>
    backend/app/config.py
    backend/app/jobs/__init__.py
    backend/app/jobs/cleanup.py
    backend/app/main.py
    backend/pyproject.toml
  </files>
  <action>
    1. Add to pyproject.toml dependencies: `apscheduler = ">=3.10.0"`

    2. Add to backend/app/config.py Settings:
       - ANONYMOUS_DATA_TTL_DAYS: int = 7  # How long to keep anonymous data
       - CLEANUP_SCHEDULE_HOUR: int = 3  # Run cleanup at 3 AM

    3. Create backend/app/jobs/__init__.py (empty file)

    4. Create backend/app/jobs/cleanup.py:
       ```python
       from datetime import datetime, timedelta, timezone
       from apscheduler.schedulers.asyncio import AsyncIOScheduler
       from app.db.neo4j_client import neo4j_driver
       from app.db.qdrant_client import qdrant_client
       from app.db.mem0_client import get_mem0
       from app.config import settings
       from qdrant_client.models import Filter, FieldCondition, MatchValue, Range
       import logging

       logger = logging.getLogger(__name__)

       # Global scheduler instance
       scheduler: AsyncIOScheduler = None

       async def cleanup_expired_anonymous_data():
           """Delete anonymous user data older than TTL.

           Runs on schedule (e.g., daily at 3 AM). Cleans:
           1. Neo4j: Documents, Chunks with anon_ user_id older than TTL
           2. Qdrant: Vectors with anon_ user_id older than TTL

           NOTE: Mem0 memories harder to clean by timestamp - rely on session expiration.

           CRITICAL: Use upload_date/created_at for TTL queries (indexed in Phase 1).
           """
           cutoff = datetime.now(timezone.utc) - timedelta(days=settings.ANONYMOUS_DATA_TTL_DAYS)
           cutoff_str = cutoff.isoformat()

           stats = {"documents": 0, "chunks": 0, "vectors": 0}

           # Step 1: Delete expired Neo4j data
           try:
               with neo4j_driver.session(database=settings.NEO4J_DATABASE) as session:
                   # Delete chunks first (they reference documents)
                   result = session.run("""
                       MATCH (d:Document)-[:CONTAINS]->(c:Chunk)
                       WHERE d.user_id STARTS WITH $prefix
                       AND d.upload_date < datetime($cutoff)
                       DETACH DELETE c
                       RETURN count(c) as count
                   """, prefix=settings.ANONYMOUS_PREFIX, cutoff=cutoff_str)

                   record = result.single()
                   if record:
                       stats["chunks"] = record["count"]

                   # Delete documents
                   result = session.run("""
                       MATCH (d:Document)
                       WHERE d.user_id STARTS WITH $prefix
                       AND d.upload_date < datetime($cutoff)
                       DETACH DELETE d
                       RETURN count(d) as count
                   """, prefix=settings.ANONYMOUS_PREFIX, cutoff=cutoff_str)

                   record = result.single()
                   if record:
                       stats["documents"] = record["count"]

               logger.info(f"Neo4j cleanup: {stats['documents']} docs, {stats['chunks']} chunks")
           except Exception as e:
               logger.error(f"Neo4j cleanup error: {e}")

           # Step 2: Delete expired Qdrant vectors
           try:
               # Get cutoff timestamp for Qdrant (uses Unix timestamp in payload)
               cutoff_ts = cutoff.timestamp()

               # Scroll to find expired anonymous vectors
               scroll_result = qdrant_client.scroll(
                   collection_name=settings.QDRANT_COLLECTION,
                   scroll_filter=Filter(
                       must=[
                           FieldCondition(
                               key="created_at",
                               range=Range(lt=cutoff_ts)
                           )
                       ]
                   ),
                   limit=1000,
                   with_payload=["user_id"],
                   with_vectors=False
               )

               # Filter to only anonymous users
               anon_points = [
                   p.id for p in scroll_result[0]
                   if p.payload.get("user_id", "").startswith(settings.ANONYMOUS_PREFIX)
               ]

               if anon_points:
                   qdrant_client.delete(
                       collection_name=settings.QDRANT_COLLECTION,
                       points_selector=anon_points
                   )
                   stats["vectors"] = len(anon_points)

               logger.info(f"Qdrant cleanup: {stats['vectors']} vectors")
           except Exception as e:
               logger.error(f"Qdrant cleanup error: {e}")

           logger.info(f"Cleanup complete: {stats}")
           return stats

       def setup_cleanup_scheduler() -> AsyncIOScheduler:
           """Initialize cleanup job scheduler."""
           global scheduler

           scheduler = AsyncIOScheduler()

           # Run daily at configured hour (default 3 AM)
           scheduler.add_job(
               cleanup_expired_anonymous_data,
               'cron',
               hour=settings.CLEANUP_SCHEDULE_HOUR,
               minute=0,
               id='cleanup_anonymous_data',
               replace_existing=True
           )

           scheduler.start()
           logger.info(f"Cleanup scheduler started, runs daily at {settings.CLEANUP_SCHEDULE_HOUR}:00")

           return scheduler

       def shutdown_cleanup_scheduler():
           """Shutdown the scheduler gracefully."""
           global scheduler
           if scheduler:
               scheduler.shutdown(wait=False)
               logger.info("Cleanup scheduler shutdown")
       ```

    5. Update backend/app/main.py lifespan:
       ```python
       from app.jobs.cleanup import setup_cleanup_scheduler, shutdown_cleanup_scheduler

       @asynccontextmanager
       async def lifespan(app: FastAPI):
           # Startup
           # ... existing startup code ...
           setup_cleanup_scheduler()

           yield

           # Shutdown
           shutdown_cleanup_scheduler()
           # ... existing shutdown code ...
       ```

    CRITICAL: Use STARTS WITH for prefix match (not CONTAINS or regex).
    CRITICAL: Delete chunks before documents (relationship order).
    NOTE: Cleanup runs async, won't block API requests.
  </action>
  <verify>
    pip install apscheduler>=3.10.0
    python -c "
from backend.app.jobs.cleanup import cleanup_expired_anonymous_data, setup_cleanup_scheduler
print('Cleanup job functions available')
print('cleanup_expired_anonymous_data:', cleanup_expired_anonymous_data)
print('setup_cleanup_scheduler:', setup_cleanup_scheduler)
"
  </verify>
  <done>
    Scheduled cleanup job created to automatically delete expired anonymous data from Neo4j and Qdrant.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create admin endpoints for shared memory management</name>
  <files>
    backend/app/services/memory_service.py
    backend/app/api/admin.py
    backend/app/main.py
  </files>
  <action>
    1. Add to backend/app/services/memory_service.py:
       ```python
       async def add_shared_memory(content: str, metadata: Optional[dict] = None) -> dict:
           """Add company-wide shared memory (MEM-05).

           ADMIN ONLY. All authenticated users can query this memory.
           Uses sentinel user_id to distinguish from user memories.
           """
           memory = get_mem0()

           result = memory.add(
               messages=content,
               user_id=settings.SHARED_MEMORY_USER_ID,
               metadata={
                   **(metadata or {}),
                   "type": "shared",
                   "scope": "company",
                   "added_at": datetime.now(timezone.utc).isoformat()
               }
           )

           return result

       async def search_with_shared_memory(
           user_id: str,
           query: str,
           limit: int = 5,
           include_shared: bool = True
       ) -> List[dict]:
           """Search user memories plus optional shared company memory.

           If include_shared=True, also searches company-wide shared memory.
           User memories prioritized over shared.
           """
           memory = get_mem0()

           # Search user's private memories
           user_results = memory.search(
               query=query,
               user_id=user_id,
               limit=limit
           )
           results = user_results.get("results", []) if isinstance(user_results, dict) else user_results

           # Mark as personal
           for r in results:
               r["is_shared"] = False

           if include_shared:
               # Also search shared company memory
               shared_results = memory.search(
                   query=query,
                   user_id=settings.SHARED_MEMORY_USER_ID,
                   limit=limit
               )
               shared_list = shared_results.get("results", []) if isinstance(shared_results, dict) else shared_results

               # Append shared results (marked as shared)
               for mem in shared_list:
                   mem["is_shared"] = True
                   results.append(mem)

           return results[:limit * 2]  # Allow more results when including shared

       async def get_shared_memories(limit: int = 50) -> List[dict]:
           """Get all shared company memories."""
           memory = get_mem0()

           results = memory.get_all(user_id=settings.SHARED_MEMORY_USER_ID, limit=limit)

           return results if isinstance(results, list) else results.get("results", [])

       async def delete_shared_memory(memory_id: str) -> bool:
           """Delete a shared memory. ADMIN ONLY."""
           memory = get_mem0()

           try:
               memory.delete(memory_id)
               return True
           except Exception:
               return False
       ```

    2. Create backend/app/api/admin.py:
       ```python
       from fastapi import APIRouter, Depends, HTTPException, status
       from app.core.rbac import require_admin
       from app.models.schemas import UserContext, MemoryAddRequest, MemoryResponse, MemoryListResponse
       from app.services.memory_service import (
           add_shared_memory, get_shared_memories, delete_shared_memory
       )

       router = APIRouter(prefix="/admin", tags=["admin"])

       @router.post("/memory/shared", response_model=dict)
       async def add_shared_memory_endpoint(
           request: MemoryAddRequest,
           current_user: UserContext = Depends(require_admin)
       ):
           """Add fact to shared company memory (MEM-05).

           ADMIN ONLY. All authenticated users can query this memory.
           """
           result = await add_shared_memory(
               content=request.content,
               metadata=request.metadata
           )

           return {
               "status": "added",
               "memory_id": result.get("id") if isinstance(result, dict) else str(result),
               "scope": "shared"
           }

       @router.get("/memory/shared", response_model=MemoryListResponse)
       async def list_shared_memories(
           limit: int = 50,
           current_user: UserContext = Depends(require_admin)
       ):
           """List all shared company memories. ADMIN ONLY."""
           results = await get_shared_memories(limit=limit)

           memories = [
               MemoryResponse(
                   id=m.get("id", ""),
                   memory=m.get("memory", ""),
                   metadata=m.get("metadata")
               )
               for m in results
           ]

           return MemoryListResponse(memories=memories, count=len(memories))

       @router.delete("/memory/shared/{memory_id}")
       async def delete_shared_memory_endpoint(
           memory_id: str,
           current_user: UserContext = Depends(require_admin)
       ):
           """Delete a shared memory. ADMIN ONLY."""
           success = await delete_shared_memory(memory_id)

           if not success:
               raise HTTPException(
                   status_code=status.HTTP_404_NOT_FOUND,
                   detail="Memory not found or already deleted"
               )

           return {"status": "deleted", "memory_id": memory_id}
       ```

    3. Update backend/app/main.py:
       - Import admin router: `from app.api.admin import router as admin_router`
       - Include router: `app.include_router(admin_router, prefix="/api/v1")`

    CRITICAL: All admin endpoints use Depends(require_admin) for role enforcement.
    NOTE: Shared memory uses sentinel SHARED_MEMORY_USER_ID ("__shared__").
  </action>
  <verify>
    # Test admin endpoints (requires admin user):
    # curl POST /api/v1/admin/memory/shared -H "Authorization: Bearer $ADMIN_TOKEN" -d '{"content":"Company policy..."}'
    # curl GET /api/v1/admin/memory/shared -H "Authorization: Bearer $ADMIN_TOKEN"
    # Non-admin should get 403
  </verify>
  <done>
    Admin endpoints created for managing shared company-wide memory with RBAC enforcement.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update memory search to include shared memories for authenticated users</name>
  <files>
    backend/app/api/memory.py
    backend/app/api/queries.py
  </files>
  <action>
    1. Update backend/app/api/memory.py search endpoint:
       - Import search_with_shared_memory
       - Modify search to use search_with_shared_memory when user is authenticated:
         ```python
         @router.post("/search", response_model=MemoryListResponse)
         async def search_memories(
             request: MemorySearchRequest,
             current_user: UserContext = Depends(get_current_user_optional)
         ):
             """Search user's memories with natural language query.

             Authenticated users: also searches shared company memories
             Anonymous users: only their session memories (no shared access)
             """
             # Anonymous users don't get shared memory access
             include_shared = not current_user.is_anonymous

             results = await search_with_shared_memory(
                 user_id=current_user.id,
                 query=request.query,
                 limit=request.limit,
                 include_shared=include_shared
             )

             memories = [
                 MemoryResponse(
                     id=m.get("id", ""),
                     memory=m.get("memory", ""),
                     metadata={
                         **(m.get("metadata") or {}),
                         "is_shared": m.get("is_shared", False)
                     },
                     score=m.get("score")
                 )
                 for m in results
             ]

             return MemoryListResponse(memories=memories, count=len(memories))
         ```

    2. Optionally update backend/app/api/queries.py to incorporate memory in query context:
       - This is a enhancement for Phase 2 - basic integration:
         ```python
         # In query endpoint, after getting document context:
         # Also get relevant memories
         from app.services.memory_service import search_with_shared_memory

         # Get memory context if user has memories
         memory_context = await search_with_shared_memory(
             user_id=current_user.id,
             query=request.query,
             limit=3,
             include_shared=not current_user.is_anonymous
         )

         # Include memory context in prompt (if relevant memories found)
         ```

    NOTE: Shared memories visible to authenticated users only (Pitfall #6).
    NOTE: Memory context in queries is optional enhancement - basic memory endpoints are the requirement.
  </action>
  <verify>
    # Test as authenticated user:
    # 1. Admin adds shared memory: "Our company uses Python 3.11"
    # 2. Regular user searches memories with query "Python version"
    # 3. Response should include shared memory with is_shared=true

    # Test as anonymous user:
    # 1. Search memories
    # 2. Should NOT include shared memories (only their session memories)
  </verify>
  <done>
    Memory search updated to include shared memories for authenticated users while excluding anonymous access.
  </done>
</task>

</tasks>

<verification>
Run the following commands to verify Phase 2 Plan 06 completion:

```bash
# 1. APScheduler installed
pip show apscheduler | grep Version

# 2. Cleanup job module works
cd backend && python -c "
from app.jobs.cleanup import cleanup_expired_anonymous_data, setup_cleanup_scheduler
print('Cleanup functions available')
"

# 3. Admin router registered
cd backend && python -c "
from app.api.admin import router
print('Admin routes:', [r.path for r in router.routes])
"

# 4. Shared memory functions available
cd backend && python -c "
from app.services.memory_service import add_shared_memory, search_with_shared_memory, get_shared_memories
print('Shared memory functions available')
"

# 5. Integration tests:
# a. Create admin user in Neo4j: MATCH (u:User {email:'admin@test.com'}) SET u.role='admin'
# b. Login as admin, add shared memory
# c. Login as regular user, search memories - should see shared
# d. As anonymous, search memories - should NOT see shared
# e. Wait for cleanup schedule or run cleanup_expired_anonymous_data() manually
```
</verification>

<success_criteria>
1. APScheduler installed and configured
2. Cleanup job runs on schedule (default: 3 AM daily)
3. Expired anonymous data deleted from Neo4j and Qdrant
4. Admin can POST to /admin/memory/shared
5. Admin can GET and DELETE shared memories
6. Non-admin users get 403 on admin endpoints
7. Authenticated users see shared memories in search results
8. Anonymous users do NOT see shared memories
9. Shared memories marked with is_shared=true in responses
</success_criteria>

<output>
After completion, create `.planning/phases/02-multi-user-memory/02-06-SUMMARY.md`
</output>
