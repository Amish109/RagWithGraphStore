---
phase: 04-langgraph-workflows
plan: 04
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - backend/app/services/memory_summarizer.py
autonomous: true

must_haves:
  truths:
    - "System detects when conversation memory exceeds token threshold"
    - "System summarizes older memories while preserving recent interactions"
    - "Critical facts (names, dates, decisions) are preserved in summaries"
    - "Summarization prevents context overflow in long conversations"
  artifacts:
    - path: "backend/app/services/memory_summarizer.py"
      provides: "Memory summarization service"
      exports: ["MemorySummarizer", "get_memory_with_summarization"]
  key_links:
    - from: "backend/app/services/memory_summarizer.py"
      to: "backend/app/db/mem0_client.py"
      via: "Mem0 client"
      pattern: "get_mem0"
    - from: "backend/app/services/memory_summarizer.py"
      to: "backend/app/services/generation_service.py"
      via: "LLM for summarization"
      pattern: "generate"
---

<objective>
Implement memory summarization service to prevent context overflow in long conversations.

Purpose: Automatically compress conversation memory when it grows too large (Success Criteria #3).
Output: Service that summarizes older memories while preserving recent interactions and critical facts.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-langgraph-workflows/04-RESEARCH.md

@backend/app/db/mem0_client.py
@backend/app/services/generation_service.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create memory summarization service</name>
  <files>backend/app/services/memory_summarizer.py</files>
  <action>
Create comprehensive memory summarization service:

1. Imports:
   - get_mem0 from db.mem0_client
   - settings from config
   - ChatOpenAI from langchain_openai (for summarization LLM)
   - datetime for timestamps

2. Define MemorySummarizer class:
   ```python
   class MemorySummarizer:
       """Manages conversation memory with automatic summarization."""

       def __init__(
           self,
           max_token_limit: int = None,
           summarization_threshold: float = None,
           recent_to_keep: int = 5
       ):
           self.mem0 = get_mem0()
           self.max_token_limit = max_token_limit or settings.MEMORY_MAX_TOKENS
           self.threshold = summarization_threshold or settings.MEMORY_SUMMARIZATION_THRESHOLD
           self.trigger_tokens = int(self.max_token_limit * self.threshold)
           self.recent_to_keep = recent_to_keep
           self.llm = ChatOpenAI(
               model=settings.OPENAI_MODEL,
               temperature=0.3  # Slightly creative for good summaries
           )

       async def add_interaction(
           self,
           user_id: str,
           query: str,
           response: str,
           session_id: str
       ) -> None:
           """Add interaction and trigger summarization if needed."""
           # Add to Mem0
           self.mem0.add(
               messages=[
                   {"role": "user", "content": query},
                   {"role": "assistant", "content": response}
               ],
               user_id=user_id,
               metadata={"session_id": session_id, "timestamp": datetime.now().isoformat()}
           )

           # Check and summarize if needed
           await self._check_and_summarize(user_id)

       async def get_memory_context(
           self,
           user_id: str,
           query: str,
           limit: int = 20
       ) -> dict:
           """Get relevant memory context for query."""
           memories = self.mem0.search(query, user_id=user_id, limit=limit)
           return {
               "memories": memories,
               "token_estimate": self._estimate_tokens(memories)
           }

       def _estimate_tokens(self, memories: list) -> int:
           """Rough token estimate (4 chars = 1 token)."""
           total_chars = sum(len(str(m.get("memory", ""))) for m in memories)
           return total_chars // 4

       async def _check_and_summarize(self, user_id: str) -> bool:
           """Check memory size and summarize if exceeding threshold."""
           memories = self.mem0.get_all(user_id=user_id)

           estimated_tokens = self._estimate_tokens(memories)

           if estimated_tokens > self.trigger_tokens:
               await self._perform_summarization(user_id, memories)
               return True
           return False

       async def _perform_summarization(self, user_id: str, memories: list) -> None:
           """Summarize older memories, keeping recent ones verbatim."""
           # Sort by timestamp if available, otherwise by order
           sorted_memories = sorted(
               memories,
               key=lambda m: m.get("metadata", {}).get("timestamp", ""),
               reverse=True
           )

           # Keep recent interactions verbatim (Pitfall #4)
           recent = sorted_memories[:self.recent_to_keep]
           to_summarize = sorted_memories[self.recent_to_keep:]

           if not to_summarize:
               return

           # Create summary preserving critical facts
           memory_text = "\n".join([str(m.get("memory", "")) for m in to_summarize])

           response = await self.llm.ainvoke([
               {
                   "role": "system",
                   "content": """Create a concise summary of the user's conversation history.
                   CRITICAL: Preserve ALL of the following if present:
                   - Names of people, places, organizations
                   - Specific dates, times, deadlines
                   - Decisions made and their reasoning
                   - User preferences and requirements
                   - Key facts and numbers

                   Format as bullet points. Be concise but comprehensive."""
               },
               {"role": "user", "content": memory_text}
           ])

           # Delete old memories
           for m in to_summarize:
               if "id" in m:
                   self.mem0.delete(m["id"])

           # Add consolidated summary
           self.mem0.add(
               f"[Historical Summary - {len(to_summarize)} interactions]:\n{response.content}",
               user_id=user_id,
               metadata={
                   "type": "summary",
                   "summarized_count": len(to_summarize),
                   "created_at": datetime.now().isoformat()
               }
           )
   ```

3. Create module-level convenience function:
   ```python
   _summarizer: Optional[MemorySummarizer] = None

   def get_memory_summarizer() -> MemorySummarizer:
       global _summarizer
       if _summarizer is None:
           _summarizer = MemorySummarizer()
       return _summarizer

   async def get_memory_with_summarization(
       user_id: str,
       query: str,
       limit: int = 20
   ) -> dict:
       """Convenience function to get memory context."""
       summarizer = get_memory_summarizer()
       return await summarizer.get_memory_context(user_id, query, limit)
   ```

CRITICAL (Pitfall #4): Always preserve recent interactions verbatim and extract critical facts.
  </action>
  <verify>
python -c "
from backend.app.services.memory_summarizer import MemorySummarizer, get_memory_with_summarization
summarizer = MemorySummarizer()
print(f'Threshold: {summarizer.trigger_tokens} tokens')
print('Memory summarizer imports OK')
"
  </verify>
  <done>memory_summarizer.py provides MemorySummarizer class with automatic summarization</done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/Desktop/RAGWithGraphStore
python -c "
from backend.app.services.memory_summarizer import MemorySummarizer, get_memory_with_summarization
from backend.app.config import settings

summarizer = MemorySummarizer()
print(f'Max tokens: {summarizer.max_token_limit}')
print(f'Threshold: {summarizer.threshold}')
print(f'Trigger at: {summarizer.trigger_tokens} tokens')
print(f'Recent to keep: {summarizer.recent_to_keep}')
print('Memory summarizer configured correctly')
"
```
</verification>

<success_criteria>
- MemorySummarizer detects when memory exceeds configured threshold
- Recent interactions are always preserved verbatim
- Summary extraction preserves names, dates, decisions
- Old memories are deleted after summarization
- Convenience function provides easy access to summarizer
</success_criteria>

<output>
After completion, create `.planning/phases/04-langgraph-workflows/04-04-SUMMARY.md`
</output>
