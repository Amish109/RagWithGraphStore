---
phase: 03-ux-streaming
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - backend/app/api/queries.py
  - backend/app/services/generation_service.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "User receives streaming responses (SSE) for queries with visible progress instead of waiting for complete answers"
    - "User sees status updates during query processing (retrieving, generating)"
    - "Streaming properly handles client disconnects without wasting resources"
  artifacts:
    - path: "backend/app/api/queries.py"
      provides: "POST /stream endpoint for SSE streaming"
      contains: "async def query_stream"
    - path: "backend/app/services/generation_service.py"
      provides: "Async streaming LLM response generator"
      exports: ["stream_answer"]
  key_links:
    - from: "backend/app/api/queries.py"
      to: "sse_starlette"
      via: "EventSourceResponse import"
      pattern: "from sse_starlette"
    - from: "backend/app/api/queries.py"
      to: "backend/app/services/generation_service.py"
      via: "stream_answer import"
      pattern: "from app.services.generation_service import.*stream_answer"
---

<objective>
Implement SSE streaming for query responses

Purpose: Users should receive tokens as they are generated instead of waiting for complete answers. This dramatically improves perceived latency and enables progress indication.

Output: SSE streaming endpoint at POST /query/stream, streaming generation service using LangChain astream()
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ux-streaming/03-RESEARCH.md

# Prior plan summaries
@.planning/phases/03-ux-streaming/03-01-SUMMARY.md

# Existing files to modify
@backend/app/api/queries.py
@backend/app/services/generation_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install sse-starlette and add streaming generation</name>
  <files>
    backend/requirements.txt
    backend/app/services/generation_service.py
  </files>
  <action>
1. Add sse-starlette to requirements.txt:
```
sse-starlette>=2.0.0
```

2. Install the dependency:
```bash
cd /Users/apple/Desktop/RAGWithGraphStore/backend
pip install sse-starlette>=2.0.0
```

3. Add streaming generation function to generation_service.py:

```python
from typing import AsyncGenerator


async def stream_answer(query: str, context: List[Dict]) -> AsyncGenerator[str, None]:
    """Stream LLM response token by token.

    Uses LangChain ChatOpenAI.astream() for async token streaming.
    CRITICAL: Enable streaming=True on the LLM for token-by-token output.

    Args:
        query: User's question.
        context: List of context chunks with 'text' and optional 'filename' keys.

    Yields:
        String chunks as they are generated by the LLM.
    """
    # Assemble context from chunks
    context_text = "\n\n".join([
        f"[Document: {chunk.get('filename', 'Unknown')}]\n{chunk['text']}"
        for chunk in context
    ])

    # Same prompt template as non-streaming
    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful document Q&A assistant. Answer questions ONLY based on the provided context.

CRITICAL INSTRUCTIONS:
- If the context does not contain information to answer the question, respond EXACTLY with: "I don't know. I couldn't find information about this in the provided documents."
- Do not use any knowledge outside the provided context
- Cite the document name when referencing information
- Be concise and direct"""),
        ("user", """Context:
{context}

Question: {query}

Answer:""")
    ])

    messages = prompt.format_messages(context=context_text, query=query)

    # Stream tokens using astream
    # Note: streaming=True must be set on LLM (already set in llm initialization)
    streaming_llm = ChatOpenAI(
        model=settings.OPENAI_MODEL,
        temperature=0,
        openai_api_key=settings.OPENAI_API_KEY,
        streaming=True,  # Enable streaming
    )

    async for chunk in streaming_llm.astream(messages):
        if chunk.content:
            yield chunk.content
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
pip show sse-starlette | grep -q "Version" && echo "sse-starlette installed"
python -c "
from app.services.generation_service import stream_answer
import inspect
assert inspect.isasyncgenfunction(stream_answer), 'stream_answer should be async generator'
print('stream_answer function exists and is async generator')
"
  </verify>
  <done>sse-starlette installed. stream_answer async generator function exists in generation_service.py using ChatOpenAI.astream().</done>
</task>

<task type="auto">
  <name>Task 2: Create SSE streaming query endpoint</name>
  <files>
    backend/app/api/queries.py
  </files>
  <action>
Update queries.py with SSE streaming endpoint:

1. Add imports at top:
```python
import json
from fastapi import Request
from sse_starlette.sse import EventSourceResponse
from app.services.generation_service import stream_answer
```

2. Add the streaming endpoint after the existing query endpoint:

```python
@router.post("/stream")
async def query_stream(
    request: Request,
    query_request: QueryRequest,
    current_user: dict = Depends(get_current_user),
):
    """Stream query response using Server-Sent Events (SSE).

    Implements QRY-02: Streaming responses with visible progress.

    SSE Event Types:
    - status: Processing stage updates ({"stage": "retrieving"|"generating"})
    - citations: Source documents found (list of citation objects)
    - token: Individual response tokens as they are generated
    - done: Stream complete signal

    Args:
        request: FastAPI request (for disconnect detection).
        query_request: QueryRequest with query string and max_results.
        current_user: Authenticated user from JWT token.

    Returns:
        EventSourceResponse streaming tokens and metadata.
    """
    user_id = current_user["id"]

    async def event_generator():
        try:
            # Step 1: Retrieve context
            yield {
                "event": "status",
                "data": json.dumps({"stage": "retrieving"})
            }

            context = await retrieve_relevant_context(
                query=query_request.query,
                user_id=user_id,
                max_results=query_request.max_results,
            )

            # Step 2: Handle no context case (QRY-04)
            if not context["chunks"]:
                yield {
                    "event": "token",
                    "data": await generate_answer_no_context()
                }
                yield {"event": "done", "data": ""}
                return

            # Step 3: Send citations
            citations = [
                {
                    "document_id": chunk["document_id"],
                    "filename": chunk["filename"],
                    "chunk_text": chunk["text"][:200] + "..." if len(chunk["text"]) > 200 else chunk["text"],
                    "relevance_score": chunk["score"]
                }
                for chunk in context["chunks"]
            ]
            yield {
                "event": "citations",
                "data": json.dumps(citations)
            }

            # Step 4: Stream LLM response
            yield {
                "event": "status",
                "data": json.dumps({"stage": "generating"})
            }

            async for token in stream_answer(query_request.query, context["chunks"]):
                # Check for client disconnect (Pitfall #5)
                if await request.is_disconnected():
                    break
                yield {"event": "token", "data": token}

            yield {"event": "done", "data": ""}

        except Exception as e:
            # Log error but send user-friendly message
            import logging
            logging.getLogger(__name__).exception(f"Streaming error: {e}")
            yield {
                "event": "error",
                "data": json.dumps({"message": "An error occurred while generating the response."})
            }

    return EventSourceResponse(
        event_generator(),
        headers={
            "X-Accel-Buffering": "no",  # Disable nginx buffering (Pitfall #1)
            "Cache-Control": "no-cache",
        }
    )
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
from app.api.queries import router
routes = [(r.path, r.methods) for r in router.routes]
print(f'Query routes: {routes}')
assert any('/stream' in path for path, _ in routes), 'Stream endpoint not found'
print('Stream endpoint registered')
"
  </verify>
  <done>POST /query/stream endpoint exists with SSE EventSourceResponse, emitting status, citations, token, and done events.</done>
</task>

<task type="auto">
  <name>Task 3: Test SSE streaming end-to-end</name>
  <files></files>
  <action>
Test the SSE streaming endpoint works correctly:

1. Verify the endpoint responds with correct content-type
2. Verify events are properly formatted
3. Verify error handling works

Note: Full integration test requires running server and authenticated user, but we can verify the implementation is correct.
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend

# Verify imports work
python -c "
from sse_starlette.sse import EventSourceResponse
from app.api.queries import query_stream
from app.services.generation_service import stream_answer
print('All streaming imports work')
"

# Verify endpoint structure
python -c "
import inspect
from app.api.queries import query_stream
sig = inspect.signature(query_stream)
params = list(sig.parameters.keys())
assert 'request' in params, 'Missing request parameter for disconnect detection'
print(f'query_stream parameters: {params}')
print('Endpoint structure correct')
"

# Verify EventSourceResponse is used
python -c "
import inspect
from app.api.queries import query_stream
source = inspect.getsource(query_stream)
assert 'EventSourceResponse' in source, 'Missing EventSourceResponse'
assert 'X-Accel-Buffering' in source, 'Missing nginx buffering header'
assert 'is_disconnected' in source, 'Missing disconnect detection'
print('SSE implementation correct')
"
  </verify>
  <done>SSE streaming endpoint correctly implements EventSourceResponse with nginx buffering disabled and client disconnect detection.</done>
</task>

</tasks>

<verification>
# Verify SSE streaming implementation
cd /Users/apple/Desktop/RAGWithGraphStore/backend

# 1. Check sse-starlette installed
pip show sse-starlette

# 2. Verify all imports
python -c "
from sse_starlette.sse import EventSourceResponse
from app.api.queries import query_stream
from app.services.generation_service import stream_answer, generate_answer
print('All imports successful')
"

# 3. Verify stream endpoint exists
python -c "
from app.api.queries import router
routes = [r.path for r in router.routes]
print(f'Routes: {routes}')
assert '/stream' in routes
print('Stream endpoint registered')
"

# 4. Verify stream_answer is async generator
python -c "
import inspect
from app.services.generation_service import stream_answer
assert inspect.isasyncgenfunction(stream_answer)
print('stream_answer is async generator')
"

# 5. Verify SSE implementation details
python -c "
import inspect
from app.api.queries import query_stream
source = inspect.getsource(query_stream)
checks = [
    ('EventSourceResponse', 'SSE response'),
    ('X-Accel-Buffering', 'nginx buffering'),
    ('is_disconnected', 'disconnect detection'),
    ('event.*status', 'status events'),
    ('event.*citations', 'citations event'),
    ('event.*token', 'token events'),
    ('event.*done', 'done event'),
]
for pattern, name in checks:
    if pattern in source:
        print(f'  {name}: OK')
    else:
        print(f'  {name}: MISSING')
"
</verification>

<success_criteria>
1. sse-starlette>=2.0.0 installed and in requirements.txt
2. stream_answer async generator exists in generation_service.py
3. POST /query/stream endpoint exists and returns EventSourceResponse
4. Streaming endpoint emits status, citations, token, and done events
5. X-Accel-Buffering: no header set to disable nginx buffering
6. Client disconnect detection with request.is_disconnected()
7. Error handling yields error event with user-friendly message
</success_criteria>

<output>
After completion, create `.planning/phases/03-ux-streaming/03-03-SUMMARY.md`
</output>
