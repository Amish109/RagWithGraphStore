---
phase: 03-ux-streaming
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/utils/task_tracker.py
  - backend/app/models/schemas.py
  - backend/app/api/documents.py
  - backend/app/services/document_processor.py
autonomous: true

must_haves:
  truths:
    - "User sees progress indicators during document upload and processing showing what stage is active"
    - "User can check processing status of uploaded documents"
    - "Task status includes current stage (extracting, chunking, embedding, indexing, summarizing)"
  artifacts:
    - path: "backend/app/utils/task_tracker.py"
      provides: "Task status tracking with TTL cleanup"
      exports: ["TaskStatus", "TaskInfo", "task_tracker"]
    - path: "backend/app/api/documents.py"
      provides: "GET /{document_id}/status endpoint"
      contains: "async def get_document_status"
  key_links:
    - from: "backend/app/services/document_processor.py"
      to: "backend/app/utils/task_tracker.py"
      via: "task_tracker.update() calls during processing"
      pattern: "task_tracker\\.update"
    - from: "backend/app/api/documents.py"
      to: "backend/app/utils/task_tracker.py"
      via: "task_tracker.get() for status endpoint"
      pattern: "task_tracker\\.get"
---

<objective>
Implement task tracking for document processing progress

Purpose: Users need visibility into document processing stages (extracting, chunking, embedding, indexing, summarizing). This enables progress indicators in the UI and is a prerequisite for document management features.

Output: TaskTracker utility with in-memory status storage, status endpoint in documents API, processing pipeline updates with stage tracking
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ux-streaming/03-RESEARCH.md

# Existing files to modify
@backend/app/models/schemas.py
@backend/app/api/documents.py
@backend/app/services/document_processor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create task tracker utility</name>
  <files>
    backend/app/utils/task_tracker.py
    backend/app/models/schemas.py
  </files>
  <action>
1. Create new file backend/app/utils/task_tracker.py:

```python
"""Task tracking for document processing with TTL cleanup.

Provides in-memory tracking of document processing stages:
- PENDING: Queued for processing
- EXTRACTING: Extracting text from PDF/DOCX
- CHUNKING: Splitting into semantic chunks
- EMBEDDING: Generating embeddings
- INDEXING: Storing in Neo4j and Qdrant
- SUMMARIZING: Generating document summary
- COMPLETED: Processing finished successfully
- FAILED: Processing failed with error

CRITICAL: Implements TTL cleanup to prevent memory exhaustion (Pitfall #3).
"""
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from threading import Lock
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)


class TaskStatus(str, Enum):
    """Document processing status stages."""
    PENDING = "pending"
    EXTRACTING = "extracting"
    CHUNKING = "chunking"
    EMBEDDING = "embedding"
    INDEXING = "indexing"
    SUMMARIZING = "summarizing"
    COMPLETED = "completed"
    FAILED = "failed"


# Progress percentages for each stage
STAGE_PROGRESS = {
    TaskStatus.PENDING: 0,
    TaskStatus.EXTRACTING: 10,
    TaskStatus.CHUNKING: 25,
    TaskStatus.EMBEDDING: 40,
    TaskStatus.INDEXING: 70,
    TaskStatus.SUMMARIZING: 85,
    TaskStatus.COMPLETED: 100,
    TaskStatus.FAILED: 0,
}


@dataclass
class TaskInfo:
    """Information about a document processing task."""
    document_id: str
    user_id: str
    filename: str
    status: TaskStatus
    progress: int
    message: str
    created_at: datetime = field(default_factory=datetime.utcnow)
    updated_at: datetime = field(default_factory=datetime.utcnow)
    error: Optional[str] = None


class TaskTracker:
    """Thread-safe task tracker with TTL cleanup.

    Usage:
        task_tracker.create(doc_id, user_id, filename)
        task_tracker.update(doc_id, TaskStatus.EXTRACTING, "Extracting text...")
        info = task_tracker.get(doc_id)
        task_tracker.fail(doc_id, "Error message")
    """

    def __init__(self, ttl_hours: int = 1):
        self._tasks: Dict[str, TaskInfo] = {}
        self._lock = Lock()
        self._ttl = timedelta(hours=ttl_hours)

    def create(self, document_id: str, user_id: str, filename: str) -> TaskInfo:
        """Create a new task in PENDING state."""
        with self._lock:
            self._cleanup_old_tasks()
            task = TaskInfo(
                document_id=document_id,
                user_id=user_id,
                filename=filename,
                status=TaskStatus.PENDING,
                progress=0,
                message="Queued for processing"
            )
            self._tasks[document_id] = task
            return task

    def update(self, document_id: str, status: TaskStatus, message: str) -> Optional[TaskInfo]:
        """Update task status and progress."""
        with self._lock:
            task = self._tasks.get(document_id)
            if task:
                task.status = status
                task.progress = STAGE_PROGRESS.get(status, 0)
                task.message = message
                task.updated_at = datetime.utcnow()
                logger.debug(f"Task {document_id}: {status.value} - {message}")
            return task

    def complete(self, document_id: str, message: str = "Processing complete") -> Optional[TaskInfo]:
        """Mark task as completed."""
        return self.update(document_id, TaskStatus.COMPLETED, message)

    def fail(self, document_id: str, error: str) -> Optional[TaskInfo]:
        """Mark task as failed with error message."""
        with self._lock:
            task = self._tasks.get(document_id)
            if task:
                task.status = TaskStatus.FAILED
                task.progress = 0
                task.message = "Processing failed"
                task.error = error
                task.updated_at = datetime.utcnow()
                logger.error(f"Task {document_id} failed: {error}")
            return task

    def get(self, document_id: str) -> Optional[TaskInfo]:
        """Get task info by document ID."""
        with self._lock:
            return self._tasks.get(document_id)

    def get_user_tasks(self, user_id: str) -> list[TaskInfo]:
        """Get all tasks for a user."""
        with self._lock:
            return [t for t in self._tasks.values() if t.user_id == user_id]

    def remove(self, document_id: str) -> None:
        """Remove a task from tracking."""
        with self._lock:
            self._tasks.pop(document_id, None)

    def _cleanup_old_tasks(self) -> None:
        """Remove tasks older than TTL. Called within lock."""
        cutoff = datetime.utcnow() - self._ttl
        to_remove = [
            doc_id for doc_id, task in self._tasks.items()
            if task.updated_at < cutoff
        ]
        for doc_id in to_remove:
            del self._tasks[doc_id]
        if to_remove:
            logger.debug(f"Cleaned up {len(to_remove)} old tasks")


# Global task tracker instance
task_tracker = TaskTracker(ttl_hours=1)
```

2. Add TaskStatusResponse schema to schemas.py:

```python
class TaskStatusResponse(BaseModel):
    """Schema for document processing status response."""
    document_id: str
    status: str
    progress: int  # 0-100
    message: str
    error: Optional[str] = None
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
from app.utils.task_tracker import task_tracker, TaskStatus
# Test create
task = task_tracker.create('test-doc', 'test-user', 'test.pdf')
assert task.status == TaskStatus.PENDING
# Test update
task_tracker.update('test-doc', TaskStatus.EXTRACTING, 'Extracting...')
info = task_tracker.get('test-doc')
assert info.status == TaskStatus.EXTRACTING
assert info.progress == 10
# Cleanup
task_tracker.remove('test-doc')
print('TaskTracker OK')
"
  </verify>
  <done>TaskTracker class exists with create, update, complete, fail, get methods. TaskStatusResponse schema added.</done>
</task>

<task type="auto">
  <name>Task 2: Add status endpoint to documents API</name>
  <files>
    backend/app/api/documents.py
  </files>
  <action>
1. Add imports at top of documents.py:
```python
from app.utils.task_tracker import task_tracker
from app.models.schemas import TaskStatusResponse
```

2. Update upload_document to create task tracking:
After `document_id = str(uuid.uuid4())` add:
```python
# Create task for status tracking
task_tracker.create(document_id, user_id, file.filename)
```

3. Add status endpoint after upload_document:

```python
@router.get("/{document_id}/status", response_model=TaskStatusResponse)
async def get_document_status(
    document_id: str,
    current_user: dict = Depends(get_current_user),
) -> TaskStatusResponse:
    """Get document processing status.

    Returns current processing stage and progress percentage.
    If document is fully processed and not in task tracker,
    returns completed status.

    Args:
        document_id: UUID of the document.
        current_user: Authenticated user from JWT.

    Returns:
        TaskStatusResponse with status, progress, and message.

    Raises:
        HTTPException 404: If document not found.
    """
    user_id = current_user["id"]

    # Check task tracker first (for in-progress documents)
    task = task_tracker.get(document_id)
    if task:
        # Verify ownership
        if task.user_id != user_id:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Document not found"
            )
        return TaskStatusResponse(
            document_id=document_id,
            status=task.status.value,
            progress=task.progress,
            message=task.message,
            error=task.error
        )

    # Check if document exists in Neo4j (already processed)
    from app.models.document import get_document_by_id
    doc = get_document_by_id(document_id, user_id)
    if doc:
        return TaskStatusResponse(
            document_id=document_id,
            status="completed",
            progress=100,
            message="Document ready"
        )

    raise HTTPException(
        status_code=status.HTTP_404_NOT_FOUND,
        detail="Document not found"
    )
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
from app.api.documents import router
routes = [r.path for r in router.routes]
assert '/{document_id}/status' in routes, f'Status route not found in {routes}'
print('Status endpoint registered')
"
  </verify>
  <done>GET /documents/{document_id}/status endpoint exists and returns TaskStatusResponse with current processing stage.</done>
</task>

<task type="auto">
  <name>Task 3: Update document processor with status updates</name>
  <files>
    backend/app/services/document_processor.py
  </files>
  <action>
Update process_document_pipeline to emit status updates at each stage:

1. Add import at top:
```python
from app.utils.task_tracker import task_tracker, TaskStatus
```

2. Update the pipeline function with status updates at each stage:

```python
async def process_document_pipeline(
    file_path: str,
    document_id: str,
    user_id: str,
    filename: str,
) -> None:
    """Complete document processing pipeline with status tracking.

    Steps (with status updates):
    1. EXTRACTING: Extract text (PDF/DOCX based on file extension)
    2. CHUNKING: Chunk text (semantic chunking)
    3. EMBEDDING: Generate embeddings for all chunks
    4. INDEXING: Store in Neo4j and Qdrant
    5. SUMMARIZING: Generate document summary
    6. COMPLETED: Processing finished

    Runs in background task to avoid blocking API (Pitfall #7).
    """
    from app.services.embedding_service import generate_embeddings
    from app.services.indexing_service import (
        store_chunks_in_qdrant,
        store_document_in_neo4j,
    )

    try:
        logger.info(f"Processing document: {filename} (id: {document_id})")

        # Step 1: Extract text
        task_tracker.update(document_id, TaskStatus.EXTRACTING, f"Extracting text from {filename}")
        _, ext = os.path.splitext(filename.lower())
        if ext == ".pdf":
            text = extract_text_from_pdf(file_path)
        elif ext in (".docx", ".doc"):
            text = extract_text_from_docx(file_path)
        else:
            raise ValueError(f"Unsupported file type: {ext}")
        logger.info(f"Extracted {len(text)} characters from {filename}")

        # Step 2: Chunk text
        task_tracker.update(document_id, TaskStatus.CHUNKING, "Splitting into semantic chunks")
        chunks = chunk_text(text)
        logger.info(f"Created {len(chunks)} chunks from {filename}")

        if not chunks:
            logger.warning(f"No chunks created from {filename} - empty document?")
            task_tracker.fail(document_id, "Document appears to be empty")
            return

        # Step 3: Generate embeddings
        task_tracker.update(document_id, TaskStatus.EMBEDDING, f"Generating embeddings for {len(chunks)} chunks")
        chunk_texts = [chunk["text"] for chunk in chunks]
        embeddings = await generate_embeddings(chunk_texts)
        logger.info(f"Generated {len(embeddings)} embeddings for {filename}")

        # Prepare chunk data with shared UUIDs
        chunk_data = []
        for chunk, embedding in zip(chunks, embeddings):
            chunk_id = str(uuid.uuid4())
            chunk_data.append({
                "id": chunk_id,
                "text": chunk["text"],
                "position": chunk["position"],
                "vector": embedding,
                "document_id": document_id,
                "user_id": user_id,
            })

        # Step 4: Store in databases
        task_tracker.update(document_id, TaskStatus.INDEXING, "Storing in database")
        store_document_in_neo4j(
            document_id=document_id,
            user_id=user_id,
            filename=filename,
            chunks=chunk_data,
        )
        logger.info(f"Stored document and chunks in Neo4j for {filename}")

        store_chunks_in_qdrant(chunk_data)
        logger.info(f"Stored vectors in Qdrant for {filename}")

        # Step 5: Generate summary (placeholder - will be implemented in Plan 04)
        task_tracker.update(document_id, TaskStatus.SUMMARIZING, "Generating document summary")
        # TODO: Add actual summarization in Plan 04
        logger.info(f"Summary generation placeholder for {filename}")

        # Step 6: Complete
        task_tracker.complete(document_id, "Document processed successfully")
        logger.info(f"Successfully processed document: {filename} (id: {document_id})")

    except Exception as e:
        logger.error(f"Error processing document {filename}: {e}")
        task_tracker.fail(document_id, str(e))
        raise

    finally:
        # Clean up temp file
        if os.path.exists(file_path):
            os.unlink(file_path)
            logger.debug(f"Cleaned up temp file: {file_path}")
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
import inspect
from app.services.document_processor import process_document_pipeline
source = inspect.getsource(process_document_pipeline)
assert 'task_tracker.update' in source, 'Missing task_tracker.update calls'
assert 'TaskStatus.EXTRACTING' in source, 'Missing EXTRACTING status'
assert 'TaskStatus.CHUNKING' in source, 'Missing CHUNKING status'
assert 'TaskStatus.EMBEDDING' in source, 'Missing EMBEDDING status'
assert 'TaskStatus.INDEXING' in source, 'Missing INDEXING status'
print('Document processor has status tracking')
"
  </verify>
  <done>process_document_pipeline emits status updates at each stage (EXTRACTING, CHUNKING, EMBEDDING, INDEXING, SUMMARIZING, COMPLETED/FAILED).</done>
</task>

</tasks>

<verification>
# Verify task tracking implementation
cd /Users/apple/Desktop/RAGWithGraphStore/backend

# 1. Check files exist
test -f app/utils/task_tracker.py && echo "task_tracker.py exists"

# 2. Verify all imports work
python -c "
from app.utils.task_tracker import task_tracker, TaskStatus, TaskInfo
from app.models.schemas import TaskStatusResponse
from app.api.documents import get_document_status
print('All imports successful')
"

# 3. Verify status endpoint registered
python -c "
from app.api.documents import router
routes = [r.path for r in router.routes]
print(f'Document routes: {routes}')
assert '/{document_id}/status' in routes
print('Status endpoint registered')
"

# 4. Verify document processor uses task tracker
python -c "
import inspect
from app.services.document_processor import process_document_pipeline
source = inspect.getsource(process_document_pipeline)
status_calls = source.count('task_tracker.')
print(f'Task tracker calls in processor: {status_calls}')
assert status_calls >= 5, 'Expected at least 5 task_tracker calls'
print('Document processor integrated with task tracker')
"
</verification>

<success_criteria>
1. TaskTracker class exists with create, update, complete, fail, get methods
2. TaskTracker implements TTL cleanup (1 hour default)
3. TaskStatusResponse schema exists with document_id, status, progress, message, error fields
4. GET /documents/{document_id}/status endpoint returns current processing stage
5. process_document_pipeline emits status updates at each processing stage
6. Progress percentages match stages: EXTRACTING=10%, CHUNKING=25%, EMBEDDING=40%, INDEXING=70%, SUMMARIZING=85%, COMPLETED=100%
</success_criteria>

<output>
After completion, create `.planning/phases/03-ux-streaming/03-02-SUMMARY.md`
</output>
