---
phase: 03-ux-streaming
plan: 04
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - backend/app/api/documents.py
  - backend/app/models/document.py
  - backend/app/models/schemas.py
  - backend/app/services/summarization_service.py
  - backend/app/services/document_processor.py
  - backend/app/services/indexing_service.py
  - backend/app/db/qdrant_client.py
autonomous: true

must_haves:
  truths:
    - "User can list all their uploaded documents with metadata (name, size, upload date)"
    - "User can delete documents and the system cascades deletion to both Neo4j and Qdrant stores"
    - "System automatically generates document summaries on upload for quick reference"
  artifacts:
    - path: "backend/app/api/documents.py"
      provides: "DELETE /{document_id} endpoint"
      contains: "async def delete_document"
    - path: "backend/app/services/summarization_service.py"
      provides: "Document summarization using LangChain"
      exports: ["generate_document_summary"]
    - path: "backend/app/db/qdrant_client.py"
      provides: "delete_by_document_id function"
      contains: "def delete_by_document_id"
  key_links:
    - from: "backend/app/api/documents.py"
      to: "backend/app/db/qdrant_client.py"
      via: "delete_by_document_id call"
      pattern: "delete_by_document_id"
    - from: "backend/app/services/document_processor.py"
      to: "backend/app/services/summarization_service.py"
      via: "generate_document_summary call"
      pattern: "generate_document_summary"
---

<objective>
Implement document management: delete with cascade and auto-summarization

Purpose: Users need to manage their documents (list, delete) and see summaries for quick reference. Delete must cascade to both Neo4j and Qdrant to maintain consistency.

Output: DELETE /documents/{id} with cascade deletion, enhanced document listing with summaries, auto-summarization during upload
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/03-ux-streaming/03-RESEARCH.md

# Prior plan summaries
@.planning/phases/03-ux-streaming/03-01-SUMMARY.md
@.planning/phases/03-ux-streaming/03-02-SUMMARY.md

# Existing files to modify
@backend/app/api/documents.py
@backend/app/models/document.py
@backend/app/models/schemas.py
@backend/app/services/document_processor.py
@backend/app/services/indexing_service.py
@backend/app/db/qdrant_client.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add cascade delete functionality</name>
  <files>
    backend/app/db/qdrant_client.py
    backend/app/models/document.py
    backend/app/api/documents.py
  </files>
  <action>
1. Add delete_by_document_id to qdrant_client.py:

```python
def delete_by_document_id(document_id: str) -> None:
    """Delete all vectors associated with a document.

    CRITICAL: Called BEFORE Neo4j deletion for consistency.
    Qdrant deletion by filter is atomic for matching points.

    Args:
        document_id: UUID of the document to delete vectors for.
    """
    from qdrant_client.models import FilterSelector

    qdrant_client.delete(
        collection_name=settings.QDRANT_COLLECTION,
        points_selector=FilterSelector(
            filter=Filter(
                must=[FieldCondition(key="document_id", match=MatchValue(value=document_id))]
            )
        ),
        wait=True,  # Wait for deletion to complete
    )
```

2. Add delete_document function to models/document.py:

```python
def delete_document(document_id: str, user_id: str) -> bool:
    """Delete a document and all its chunks from Neo4j.

    Uses DETACH DELETE to cascade deletion to all chunks.
    CRITICAL: Always filter by user_id for multi-tenant isolation.

    Args:
        document_id: UUID of the document.
        user_id: ID of the requesting user (for ownership verification).

    Returns:
        True if document was deleted, False if not found/not owned.
    """
    with neo4j_driver.session(database=settings.NEO4J_DATABASE) as session:
        result = session.run(
            """
            MATCH (u:User {id: $user_id})-[:OWNS]->(d:Document {id: $document_id})
            OPTIONAL MATCH (d)-[:CONTAINS]->(c:Chunk)
            WITH d, collect(c) as chunks
            DETACH DELETE d
            FOREACH (chunk IN chunks | DELETE chunk)
            RETURN count(d) as deleted
            """,
            document_id=document_id,
            user_id=user_id,
        )
        record = result.single()
        return record and record["deleted"] > 0
```

3. Add DELETE endpoint to api/documents.py:

Add import at top:
```python
from app.db.qdrant_client import delete_by_document_id
from app.models.document import delete_document
```

Add endpoint:
```python
@router.delete("/{document_id}", response_model=MessageResponse)
async def delete_document_endpoint(
    document_id: str,
    current_user: dict = Depends(get_current_user),
) -> MessageResponse:
    """Delete a document and all associated data.

    Implements MGMT-02: Document deletion with cascade.

    CRITICAL: Order matters for consistency (Pitfall #2):
    1. Verify ownership in Neo4j
    2. Delete from Qdrant first (no rollback available)
    3. Delete from Neo4j (transactional)

    If Neo4j deletion fails after Qdrant, vectors are orphaned
    but that's safer than orphaned Neo4j data with missing vectors.

    Args:
        document_id: UUID of the document to delete.
        current_user: Authenticated user from JWT.

    Returns:
        MessageResponse confirming deletion.

    Raises:
        HTTPException 404: If document not found or not owned by user.
    """
    user_id = current_user["id"]

    # Step 1: Verify ownership
    doc = get_document_by_id(document_id, user_id)
    if not doc:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Document not found"
        )

    # Step 2: Delete from Qdrant first
    # (Qdrant has no transaction support, so do it first)
    delete_by_document_id(document_id)

    # Step 3: Delete from Neo4j
    deleted = delete_document(document_id, user_id)
    if not deleted:
        # This shouldn't happen if ownership check passed,
        # but handle gracefully
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to delete document from database"
        )

    # Clean up task tracker if still present
    task_tracker.remove(document_id)

    return MessageResponse(message="Document deleted successfully")
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
from app.db.qdrant_client import delete_by_document_id
from app.models.document import delete_document
from app.api.documents import router
routes = [r.path for r in router.routes if 'DELETE' in str(r.methods)]
print(f'DELETE routes: {routes}')
assert '/{document_id}' in [r.path for r in router.routes]
print('Delete endpoint registered')
"
  </verify>
  <done>DELETE /documents/{document_id} endpoint exists with cascade deletion to both Qdrant (first) and Neo4j (second).</done>
</task>

<task type="auto">
  <name>Task 2: Create summarization service and update schemas</name>
  <files>
    backend/app/services/summarization_service.py
    backend/app/models/schemas.py
    backend/app/services/indexing_service.py
  </files>
  <action>
1. Create new file backend/app/services/summarization_service.py:

```python
"""Document summarization service using LangChain.

Generates concise summaries for uploaded documents using map_reduce
chain for scalability with large documents (Pitfall #4).
"""
import logging
from typing import List

from langchain.chains.summarize import load_summarize_chain
from langchain_core.documents import Document as LCDocument
from langchain_openai import ChatOpenAI

from app.config import settings

logger = logging.getLogger(__name__)


async def generate_document_summary(chunks: List[str], max_length: int = 500) -> str:
    """Generate a summary of a document from its chunks.

    Uses map_reduce chain for large documents to avoid token limits.
    For small documents (<=4 chunks), uses stuff chain for efficiency.

    CRITICAL: Uses map_reduce for >4 chunks to prevent context overflow (Pitfall #4).

    Args:
        chunks: List of text chunks from the document.
        max_length: Maximum summary length in characters (approximate).

    Returns:
        Summary string, or empty string if summarization fails.
    """
    if not chunks:
        return ""

    try:
        # Convert to LangChain Document format
        lc_docs = [LCDocument(page_content=chunk) for chunk in chunks]

        # Initialize LLM for summarization
        llm = ChatOpenAI(
            model=settings.OPENAI_MODEL,
            temperature=0,
            openai_api_key=settings.OPENAI_API_KEY,
        )

        # Choose chain type based on document size
        # map_reduce handles large docs by summarizing chunks first, then combining
        if len(lc_docs) <= 4:
            chain = load_summarize_chain(llm, chain_type="stuff")
        else:
            chain = load_summarize_chain(llm, chain_type="map_reduce")

        # Generate summary
        result = await chain.ainvoke({"input_documents": lc_docs})
        summary = result.get("output_text", "")

        # Truncate if needed
        if len(summary) > max_length:
            summary = summary[:max_length-3] + "..."

        logger.info(f"Generated summary: {len(summary)} chars from {len(chunks)} chunks")
        return summary

    except Exception as e:
        logger.error(f"Summarization failed: {e}")
        return ""
```

2. Update DocumentInfo schema in schemas.py to include summary:

Find the DocumentInfo class and update it:
```python
class DocumentInfo(BaseModel):
    """Schema for document information."""
    id: str
    filename: str
    upload_date: Optional[str] = None
    chunk_count: Optional[int] = None
    summary: Optional[str] = None  # Add this field
```

3. Update store_document_in_neo4j in indexing_service.py to accept summary:

Update the function signature and Cypher query to store summary:
```python
def store_document_in_neo4j(
    document_id: str,
    user_id: str,
    filename: str,
    chunks: List[Dict],
    summary: str = "",  # Add summary parameter
) -> None:
    """Store document metadata and chunks in Neo4j.
    ...
    """
    with neo4j_driver.session(database=settings.NEO4J_DATABASE) as session:
        # Create or get user, create document with summary
        session.run(
            """
            MERGE (u:User {id: $user_id})
            CREATE (d:Document {
                id: $document_id,
                filename: $filename,
                upload_date: datetime(),
                chunk_count: $chunk_count,
                summary: $summary
            })
            CREATE (u)-[:OWNS]->(d)
            """,
            user_id=user_id,
            document_id=document_id,
            filename=filename,
            chunk_count=len(chunks),
            summary=summary,  # Store summary
        )
        # ... rest of chunk creation
```

4. Update get_document_by_id and get_user_documents in models/document.py to return summary:

```python
# In get_document_by_id and get_user_documents, update the RETURN clause:
RETURN d {
    .id,
    .filename,
    .upload_date,
    .chunk_count,
    .summary
} AS document
```
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
from app.services.summarization_service import generate_document_summary
from app.models.schemas import DocumentInfo
import inspect

# Check summarization service
assert inspect.iscoroutinefunction(generate_document_summary)
print('Summarization service OK')

# Check DocumentInfo has summary
doc = DocumentInfo(id='test', filename='test.pdf', summary='Test summary')
assert doc.summary == 'Test summary'
print('DocumentInfo schema has summary field')
"
  </verify>
  <done>summarization_service.py exists with generate_document_summary using map_reduce chain. DocumentInfo schema includes summary field. Neo4j storage includes summary.</done>
</task>

<task type="auto">
  <name>Task 3: Integrate summarization into document processing</name>
  <files>
    backend/app/services/document_processor.py
  </files>
  <action>
Update process_document_pipeline to generate and store summaries:

1. Add import at top:
```python
from app.services.summarization_service import generate_document_summary
```

2. Update the pipeline to generate summary before indexing. Find the SUMMARIZING stage and replace the placeholder:

```python
# Step 5: Generate summary
task_tracker.update(document_id, TaskStatus.SUMMARIZING, "Generating document summary")
summary = await generate_document_summary(chunk_texts)
logger.info(f"Generated summary for {filename}: {len(summary)} chars")
```

3. Pass summary to store_document_in_neo4j:

```python
# Step 4: Store in databases
task_tracker.update(document_id, TaskStatus.INDEXING, "Storing in database")
store_document_in_neo4j(
    document_id=document_id,
    user_id=user_id,
    filename=filename,
    chunks=chunk_data,
    summary=summary,  # Add summary parameter
)
```

Note: The order should be:
1. EXTRACTING
2. CHUNKING
3. EMBEDDING
4. SUMMARIZING (before indexing, so we have summary to store)
5. INDEXING
6. COMPLETED
  </action>
  <verify>
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "
import inspect
from app.services.document_processor import process_document_pipeline
source = inspect.getsource(process_document_pipeline)
assert 'generate_document_summary' in source, 'Missing summarization call'
assert 'summary=' in source, 'Missing summary parameter in store call'
print('Document processor integrates summarization')
"
  </verify>
  <done>process_document_pipeline generates summaries using generate_document_summary and stores them in Neo4j via store_document_in_neo4j(summary=...).</done>
</task>

</tasks>

<verification>
# Verify document management implementation
cd /Users/apple/Desktop/RAGWithGraphStore/backend

# 1. Check all files exist
test -f app/services/summarization_service.py && echo "summarization_service.py exists"

# 2. Verify delete endpoint
python -c "
from app.api.documents import router
routes = [(r.path, list(r.methods)) for r in router.routes]
print(f'Document routes: {routes}')
delete_routes = [r for r in routes if 'DELETE' in r[1]]
assert len(delete_routes) > 0, 'No DELETE routes found'
print('Delete endpoint exists')
"

# 3. Verify cascade delete functions
python -c "
from app.db.qdrant_client import delete_by_document_id
from app.models.document import delete_document
print('Cascade delete functions exist')
"

# 4. Verify summarization service
python -c "
from app.services.summarization_service import generate_document_summary
import inspect
assert inspect.iscoroutinefunction(generate_document_summary)
print('Summarization service is async')
"

# 5. Verify DocumentInfo has summary
python -c "
from app.models.schemas import DocumentInfo
doc = DocumentInfo(id='x', filename='x.pdf', summary='test')
print(f'DocumentInfo with summary: {doc.model_dump()}')
"

# 6. Verify document processor uses summarization
python -c "
import inspect
from app.services.document_processor import process_document_pipeline
source = inspect.getsource(process_document_pipeline)
assert 'generate_document_summary' in source
print('Document processor uses summarization')
"
</verification>

<success_criteria>
1. DELETE /documents/{document_id} endpoint exists
2. Cascade deletion removes data from Qdrant first, then Neo4j
3. summarization_service.py exists with generate_document_summary function
4. Summarization uses map_reduce for documents with >4 chunks
5. DocumentInfo schema includes optional summary field
6. Document processing pipeline generates summary during SUMMARIZING stage
7. Summaries stored in Neo4j Document nodes
8. Document listing returns summaries in response
</success_criteria>

<output>
After completion, create `.planning/phases/03-ux-streaming/03-04-SUMMARY.md`
</output>
