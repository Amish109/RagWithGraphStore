---
phase: 06-production-hardening
plan: 05
type: execute
wave: 3
depends_on: ["06-01"]
files_modified:
  - backend/requirements.txt
  - backend/app/evaluation/__init__.py
  - backend/app/evaluation/rag_metrics.py
  - backend/app/evaluation/dataset.py
  - backend/app/api/admin.py
autonomous: true

must_haves:
  truths:
    - "RAGAs metrics evaluate retrieval quality (faithfulness, context recall)"
    - "Evaluation can be triggered via admin API endpoint"
    - "Sample evaluation dataset exists with ground truth"
  artifacts:
    - path: "backend/app/evaluation/rag_metrics.py"
      provides: "RAGAs integration for evaluation"
      contains: "evaluate"
    - path: "backend/app/evaluation/dataset.py"
      provides: "Sample evaluation dataset"
      contains: "ground_truth"
  key_links:
    - from: "backend/app/api/admin.py"
      to: "backend/app/evaluation/rag_metrics.py"
      via: "run_evaluation import"
      pattern: "from app.evaluation"
---

<objective>
Implement RAG evaluation framework using RAGAs to track retrieval accuracy and response quality.

Purpose: Enable objective measurement of RAG system quality through standardized metrics (faithfulness, context recall, factual correctness). This satisfies Success Criteria #6: comprehensive evaluation framework.
Output: Working evaluation infrastructure with sample dataset and admin API endpoint to trigger evaluation runs.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-production-hardening/06-RESEARCH.md
@.planning/phases/06-production-hardening/06-01-SUMMARY.md

# Reference for existing services
@backend/app/services/retrieval_service.py
@backend/app/api/admin.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add RAGAs dependency and create evaluation module structure</name>
  <files>backend/requirements.txt, backend/app/evaluation/__init__.py</files>
  <action>
1. Add to requirements.txt:
   - ragas>=0.2.0
   - datasets  # Required by RAGAs for Dataset creation

2. Create backend/app/evaluation/ directory

3. Create backend/app/evaluation/__init__.py with:
   """RAG evaluation module using RAGAs framework."""
   from app.evaluation.rag_metrics import evaluate_rag_quality, EvaluationResult
   from app.evaluation.dataset import load_evaluation_dataset, EvaluationSample

   __all__ = [
       "evaluate_rag_quality",
       "EvaluationResult",
       "load_evaluation_dataset",
       "EvaluationSample",
   ]
  </action>
  <verify>pip install -r backend/requirements.txt; python -c "import ragas; print(ragas.__version__)"</verify>
  <done>RAGAs installed, evaluation module structure created</done>
</task>

<task type="auto">
  <name>Task 2: Create RAGAs evaluation service</name>
  <files>backend/app/evaluation/rag_metrics.py</files>
  <action>
Create backend/app/evaluation/rag_metrics.py with:

1. Imports:
   - ragas: evaluate
   - ragas.metrics: faithfulness, context_recall (or context_precision based on RAGAs version)
   - datasets: Dataset
   - pydantic: BaseModel
   - typing: List, Optional
   - structlog for logging

2. EvaluationResult Pydantic model:
   class EvaluationResult(BaseModel):
       """Results from RAG evaluation run."""
       faithfulness: float  # Is answer grounded in context?
       context_recall: float  # Did we retrieve relevant info?
       overall_score: float  # Average of metrics
       num_samples: int
       failed_samples: int
       details: Optional[list] = None  # Per-sample results if requested

3. evaluate_rag_quality async function:
   async def evaluate_rag_quality(
       test_cases: list[dict],
       include_details: bool = False,
   ) -> EvaluationResult:
       """
       Evaluate RAG system quality using RAGAs metrics.

       Args:
           test_cases: List of dicts with keys:
               - question: str (the query)
               - answer: str (generated answer)
               - contexts: list[str] (retrieved context chunks)
               - ground_truth: str (expected answer for comparison)
           include_details: Include per-sample breakdown

       Returns:
           EvaluationResult with aggregated metrics.

       Note: This function is CPU/LLM intensive. Run async but expect
       it to take significant time for large datasets.
       """
       logger = structlog.get_logger()
       await logger.ainfo("evaluation_started", num_samples=len(test_cases))

       # Convert to HuggingFace Dataset format
       # RAGAs expects specific column names
       dataset = Dataset.from_list([
           {
               "question": tc["question"],
               "answer": tc["answer"],
               "contexts": tc["contexts"],
               "ground_truth": tc.get("ground_truth", ""),
           }
           for tc in test_cases
       ])

       try:
           # Run evaluation with selected metrics
           # Note: RAGAs 0.2+ API may differ - check actual version
           result = evaluate(
               dataset,
               metrics=[faithfulness, context_recall],
           )

           faithfulness_score = float(result.get("faithfulness", 0))
           context_recall_score = float(result.get("context_recall", 0))
           overall = (faithfulness_score + context_recall_score) / 2

           await logger.ainfo(
               "evaluation_complete",
               faithfulness=faithfulness_score,
               context_recall=context_recall_score,
               overall=overall,
           )

           return EvaluationResult(
               faithfulness=faithfulness_score,
               context_recall=context_recall_score,
               overall_score=overall,
               num_samples=len(test_cases),
               failed_samples=0,
               details=result.to_pandas().to_dict("records") if include_details else None,
           )

       except Exception as e:
           await logger.aerror("evaluation_failed", error=str(e))
           raise

CRITICAL: RAGAs uses LLM calls internally - this is expensive. Use sparingly.
AVOID: Running on production traffic - use dedicated evaluation dataset.
  </action>
  <verify>python -c "from app.evaluation.rag_metrics import evaluate_rag_quality, EvaluationResult; print('RAG metrics module loads')"</verify>
  <done>RAGAs evaluation service created with EvaluationResult model</done>
</task>

<task type="auto">
  <name>Task 3: Create sample evaluation dataset and admin endpoint</name>
  <files>backend/app/evaluation/dataset.py, backend/app/api/admin.py</files>
  <action>
1. Create backend/app/evaluation/dataset.py with:

   from pydantic import BaseModel
   from typing import List

   class EvaluationSample(BaseModel):
       """Single evaluation test case."""
       question: str
       ground_truth: str
       contexts: List[str] = []  # Populated during evaluation
       answer: str = ""  # Populated during evaluation

   # Sample evaluation dataset - expand with domain-specific examples
   SAMPLE_EVALUATION_DATASET: List[EvaluationSample] = [
       EvaluationSample(
           question="What is retrieval augmented generation?",
           ground_truth="RAG combines retrieval of relevant documents with language model generation to produce accurate, grounded responses.",
       ),
       EvaluationSample(
           question="How does vector search work?",
           ground_truth="Vector search uses embeddings to find semantically similar content by measuring distance in high-dimensional space.",
       ),
       EvaluationSample(
           question="What is a knowledge graph?",
           ground_truth="A knowledge graph represents information as nodes (entities) and edges (relationships) to enable structured querying and reasoning.",
       ),
       # Add more samples as the system matures
       # Minimum 10-20 for meaningful metrics, 50+ ideal
   ]

   def load_evaluation_dataset() -> List[EvaluationSample]:
       """Load the evaluation dataset."""
       return SAMPLE_EVALUATION_DATASET

   def add_evaluation_sample(sample: EvaluationSample) -> None:
       """Add sample to dataset (in-memory for now, could persist to DB)."""
       SAMPLE_EVALUATION_DATASET.append(sample)

2. Modify backend/app/api/admin.py to add evaluation endpoint:

   Add imports:
   from app.evaluation import evaluate_rag_quality, load_evaluation_dataset, EvaluationResult
   from app.services.retrieval_service import retrieve_context
   from app.services.generation_service import generate_answer  # or equivalent

   Add endpoint:
   @router.post("/admin/evaluate", response_model=EvaluationResult)
   async def run_evaluation(
       include_details: bool = False,
       current_user: User = Depends(require_admin),
   ):
       """
       Run RAG evaluation against sample dataset.

       ADMIN ONLY: This endpoint is expensive (LLM calls for each sample).
       Use sparingly for periodic quality assessment.

       Args:
           include_details: Include per-sample breakdown in response

       Returns:
           EvaluationResult with faithfulness, context_recall, and overall score.
       """
       logger = structlog.get_logger()
       await logger.ainfo("admin_evaluation_started", user_id=current_user.id)

       # Load evaluation samples
       samples = load_evaluation_dataset()

       # Generate answers for each sample using the real system
       test_cases = []
       for sample in samples:
           # Retrieve context for the question
           contexts = await retrieve_context(
               query=sample.question,
               user_id=current_user.id,  # Use admin's context
               max_results=5,
           )
           context_texts = [c.text for c in contexts]

           # Generate answer
           answer = await generate_answer(
               query=sample.question,
               context=context_texts,
           )

           test_cases.append({
               "question": sample.question,
               "answer": answer,
               "contexts": context_texts,
               "ground_truth": sample.ground_truth,
           })

       # Run evaluation
       result = await evaluate_rag_quality(test_cases, include_details=include_details)

       await logger.ainfo(
           "admin_evaluation_complete",
           overall_score=result.overall_score,
           num_samples=result.num_samples,
       )

       return result

Note: Adapt the retrieve_context and generate_answer calls to match actual service interfaces in the codebase.

CRITICAL: Admin-only endpoint due to cost (multiple LLM calls per sample).
CRITICAL: Log evaluation runs for tracking quality over time.
  </action>
  <verify>python -c "from app.evaluation.dataset import load_evaluation_dataset; print(f'Loaded {len(load_evaluation_dataset())} samples')"</verify>
  <done>Sample evaluation dataset created, admin endpoint added for triggering evaluations</done>
</task>

</tasks>

<verification>
1. Module loads:
   - python -c "from app.evaluation import evaluate_rag_quality, load_evaluation_dataset"

2. Sample dataset:
   - At least 3-5 samples with question/ground_truth pairs
   - Samples cover different query types

3. Admin endpoint (requires running app):
   - POST /api/v1/admin/evaluate requires admin auth
   - Returns EvaluationResult with metrics
   - Logs evaluation run

4. Code inspection:
   - EvaluationResult model has all required fields
   - Admin endpoint properly protected
</verification>

<success_criteria>
- RAGAs library installed and importable
- evaluate_rag_quality function accepts test cases and returns metrics
- EvaluationResult includes faithfulness, context_recall, overall_score
- Sample evaluation dataset with at least 3 ground-truth examples
- Admin endpoint /admin/evaluate triggers full evaluation pipeline
- Evaluation runs logged for observability
</success_criteria>

<output>
After completion, create `.planning/phases/06-production-hardening/06-05-SUMMARY.md`
</output>
