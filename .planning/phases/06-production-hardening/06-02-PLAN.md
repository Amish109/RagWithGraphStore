---
phase: 06-production-hardening
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/requirements.txt
  - backend/app/config.py
  - backend/app/core/metrics.py
  - backend/app/core/rate_limiter.py
  - backend/app/main.py
  - backend/app/api/queries.py
  - backend/app/api/documents.py
autonomous: true

must_haves:
  truths:
    - "Prometheus metrics are exposed at /metrics endpoint"
    - "Rate limiting prevents more than N requests per minute per user/IP"
    - "Rate limit exceeded returns 429 with Retry-After header"
  artifacts:
    - path: "backend/app/core/metrics.py"
      provides: "Prometheus metrics configuration"
      contains: "Instrumentator"
    - path: "backend/app/core/rate_limiter.py"
      provides: "slowapi rate limiter with Redis backend"
      contains: "Limiter"
  key_links:
    - from: "backend/app/main.py"
      to: "backend/app/core/metrics.py"
      via: "instrument_app() call"
      pattern: "instrument.*app"
    - from: "backend/app/api/queries.py"
      to: "backend/app/core/rate_limiter.py"
      via: "@limiter.limit decorator"
      pattern: "@limiter\\.limit"
---

<objective>
Add Prometheus metrics for monitoring and rate limiting for cost protection.

Purpose: Provide operational visibility through metrics and prevent runaway API costs from abuse or accidents through per-user rate limiting backed by existing Redis infrastructure.
Output: Working /metrics endpoint exposing standard HTTP metrics plus custom LLM token counter, and rate limiting on expensive endpoints.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-production-hardening/06-RESEARCH.md

# Existing files to modify
@backend/app/main.py
@backend/app/config.py
@backend/app/db/redis_client.py
@backend/app/api/queries.py
@backend/app/api/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add metrics and rate limiting dependencies and configuration</name>
  <files>backend/requirements.txt, backend/app/config.py</files>
  <action>
1. Add to requirements.txt (if not already present):
   - prometheus-fastapi-instrumentator
   - slowapi
   - limits

2. Add to config.py Settings class:
   - RATE_LIMIT_ENABLED: bool = True
   - RATE_LIMIT_QUERY: str = "20/minute" (queries per minute)
   - RATE_LIMIT_UPLOAD: str = "5/minute" (uploads per minute - more expensive)
   - RATE_LIMIT_DEFAULT: str = "60/minute" (default for other endpoints)
   - METRICS_ENABLED: bool = True

Note: Rate limits use slowapi format: "N/second", "N/minute", "N/hour", "N/day"
  </action>
  <verify>pip install -r backend/requirements.txt; python -c "from app.config import settings; print(settings.RATE_LIMIT_QUERY)"</verify>
  <done>Dependencies installed, rate limit config accessible</done>
</task>

<task type="auto">
  <name>Task 2: Create Prometheus metrics module with custom LLM token counter</name>
  <files>backend/app/core/metrics.py</files>
  <action>
Create backend/app/core/metrics.py with:

1. Import prometheus_fastapi_instrumentator.Instrumentator and prometheus_client.Counter

2. Create custom metric:
   llm_tokens_total = Counter(
       "llm_tokens_total",
       "Total LLM tokens used",
       labelnames=("model", "operation")  # operation: embedding, completion, etc.
   )

3. Create llm_requests_total Counter with labels (model, operation, status)

4. Create setup_metrics(app: FastAPI) function that:
   - Checks settings.METRICS_ENABLED (skip if False)
   - Creates Instrumentator with excluded_handlers=["/metrics", "/health"]
   - Sets should_group_status_codes=True
   - Calls instrumentator.instrument(app)
   - Calls instrumentator.expose(app, include_in_schema=False)
   - Logs "Prometheus metrics enabled at /metrics"

5. Create helper functions:
   - record_llm_tokens(model: str, operation: str, count: int) - increments llm_tokens_total
   - record_llm_request(model: str, operation: str, success: bool) - increments llm_requests_total

AVOID: High-cardinality labels like user_id, query text, or request_id in metrics (causes memory explosion).
  </action>
  <verify>python -c "from app.core.metrics import llm_tokens_total, record_llm_tokens; record_llm_tokens('gpt-4o', 'completion', 100)"</verify>
  <done>Prometheus metrics module created with HTTP auto-instrumentation and custom LLM counters</done>
</task>

<task type="auto">
  <name>Task 3: Create rate limiter and apply to API endpoints</name>
  <files>backend/app/core/rate_limiter.py, backend/app/main.py, backend/app/api/queries.py, backend/app/api/documents.py</files>
  <action>
1. Create backend/app/core/rate_limiter.py with:

   from slowapi import Limiter
   from slowapi.util import get_remote_address
   from slowapi.errors import RateLimitExceeded
   from fastapi import Request
   from fastapi.responses import JSONResponse

   def get_rate_limit_key(request: Request) -> str:
       """Composite key: user_id for authenticated, IP for anonymous.

       CRITICAL: Use composite key to prevent rate limit collision between users.
       """
       user = getattr(request.state, "user", None)
       if user and hasattr(user, "id") and not getattr(user, "is_anonymous", True):
           return f"user:{user.id}"
       return f"ip:{request.client.host}"

   # Build Redis URI for async storage
   # slowapi requires async+redis:// prefix for async support
   redis_uri = settings.REDIS_URL.replace("redis://", "async+redis://")

   limiter = Limiter(
       key_func=get_rate_limit_key,
       storage_uri=redis_uri,
       enabled=settings.RATE_LIMIT_ENABLED,
   )

   async def rate_limit_exceeded_handler(request: Request, exc: RateLimitExceeded) -> JSONResponse:
       """Return 429 with Retry-After header."""
       return JSONResponse(
           status_code=429,
           content={
               "error": "rate_limited",
               "message": f"Rate limit exceeded. Try again in {exc.detail} seconds.",
           },
           headers={"Retry-After": str(exc.detail)},
       )

2. Modify backend/app/main.py:
   - Import limiter, rate_limit_exceeded_handler from app.core.rate_limiter
   - Import setup_metrics from app.core.metrics
   - Add: app.state.limiter = limiter
   - Add: app.add_exception_handler(RateLimitExceeded, rate_limit_exceeded_handler)
   - Call setup_metrics(app) after app creation

3. Modify backend/app/api/queries.py:
   - Import limiter from app.core.rate_limiter
   - Add @limiter.limit(settings.RATE_LIMIT_QUERY) to POST /query endpoint
   - Add @limiter.limit(settings.RATE_LIMIT_QUERY) to POST /enhanced endpoint
   - Add Request parameter to endpoints for rate limiter access

4. Modify backend/app/api/documents.py:
   - Import limiter from app.core.rate_limiter
   - Add @limiter.limit(settings.RATE_LIMIT_UPLOAD) to POST /documents/upload endpoint
   - Add Request parameter to endpoint for rate limiter access

CRITICAL: The @limiter.limit decorator must come AFTER @router.post decorator.
CRITICAL: Endpoints must accept Request as first parameter for limiter to work.
  </action>
  <verify>
1. Start app: uvicorn app.main:app --port 8000 &
2. Test rate limit: for i in {1..25}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:8000/api/v1/query -X POST -H "Content-Type: application/json" -d '{"query":"test"}'; done
3. Should see 429 responses after 20 requests
4. Check /metrics endpoint returns Prometheus format
5. Kill uvicorn
  </verify>
  <done>Rate limiting active on query and upload endpoints, /metrics endpoint exposed</done>
</task>

</tasks>

<verification>
1. Metrics endpoint test:
   - curl http://localhost:8000/metrics returns Prometheus text format
   - Contains http_request_duration_seconds histogram
   - Contains http_requests_total counter

2. Rate limiting test:
   - Rapid requests to /query return 429 after limit exceeded
   - Response includes Retry-After header
   - Different users (different auth tokens) have separate limits

3. Code inspection:
   - No high-cardinality labels (user_id, query text) in metrics
   - Rate limit key uses composite user:id or ip:host format
</verification>

<success_criteria>
- /metrics endpoint exposes Prometheus-format metrics
- HTTP request duration and count metrics auto-collected
- Custom llm_tokens_total counter available for LLM tracking
- Rate limiting enforced: 20/min for queries, 5/min for uploads
- 429 responses include Retry-After header
- Rate limits per-user for authenticated, per-IP for anonymous
</success_criteria>

<output>
After completion, create `.planning/phases/06-production-hardening/06-02-SUMMARY.md`
</output>
