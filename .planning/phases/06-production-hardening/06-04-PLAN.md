---
phase: 06-production-hardening
plan: 04
type: execute
wave: 3
depends_on: ["06-01", "06-02", "06-03"]
files_modified:
  - backend/tests/load/locustfile.py
  - backend/tests/load/README.md
autonomous: false

must_haves:
  truths:
    - "Load test simulates realistic user behavior with authentication"
    - "System maintains sub-2s response times with 100+ concurrent users"
    - "Load test results show p50, p95, p99 latency metrics"
  artifacts:
    - path: "backend/tests/load/locustfile.py"
      provides: "Locust load test configuration"
      contains: "HttpUser"
    - path: "backend/tests/load/README.md"
      provides: "Load testing instructions"
      contains: "locust"
  key_links:
    - from: "backend/tests/load/locustfile.py"
      to: "/api/v1/query"
      via: "self.client.post"
      pattern: "client\\.post.*query"
---

<objective>
Create load testing infrastructure with Locust to verify system performance under concurrent load.

Purpose: Validate that the system meets Success Criteria #1 and #4: sub-2s response times with 100+ concurrent users. Identify bottlenecks before production deployment.
Output: Runnable Locust load tests that simulate realistic RAG usage patterns with authentication.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-production-hardening/06-RESEARCH.md
@.planning/phases/06-production-hardening/06-01-SUMMARY.md
@.planning/phases/06-production-hardening/06-02-SUMMARY.md
@.planning/phases/06-production-hardening/06-03-SUMMARY.md

# Reference for API structure
@backend/app/api/queries.py
@backend/app/api/documents.py
@backend/app/api/auth.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Locust load test file with realistic user scenarios</name>
  <files>backend/tests/load/locustfile.py</files>
  <action>
Create backend/tests/load/ directory and backend/tests/load/locustfile.py with:

1. Import statements:
   - locust: HttpUser, task, between, events
   - json, random, os

2. Sample query list (10-15 realistic questions):
   SAMPLE_QUERIES = [
       "What are the main findings?",
       "Summarize the key points",
       "What conclusions were drawn?",
       "Explain the methodology used",
       "What are the recommendations?",
       ...
   ]

3. RAGUser class (HttpUser):
   - wait_time = between(1, 5)  # Realistic user think time
   - host attribute from environment or default to localhost:8000

   def on_start(self):
       """Authenticate user on test start."""
       # Use test credentials from environment
       email = os.getenv("LOAD_TEST_EMAIL", "loadtest@example.com")
       password = os.getenv("LOAD_TEST_PASSWORD", "loadtestpassword")

       response = self.client.post(
           "/api/v1/auth/login",
           data={"username": email, "password": password},
           name="/auth/login"
       )

       if response.status_code == 200:
           self.token = response.json().get("access_token")
           self.headers = {"Authorization": f"Bearer {self.token}"}
       else:
           # Fall back to anonymous testing
           self.token = None
           self.headers = {}

   @task(5)  # Most common operation
   def query_documents(self):
       """Simulate document query."""
       query = random.choice(SAMPLE_QUERIES)
       self.client.post(
           "/api/v1/query/",
           json={"query": query, "max_results": 5},
           headers=self.headers,
           name="/query (basic)"
       )

   @task(2)  # Less common - enhanced query
   def query_enhanced(self):
       """Simulate enhanced query with confidence scores."""
       query = random.choice(SAMPLE_QUERIES)
       self.client.post(
           "/api/v1/query/enhanced",
           json={"query": query, "max_results": 3},
           headers=self.headers,
           name="/query/enhanced"
       )

   @task(1)  # Rare - health check
   def health_check(self):
       """Health check endpoint."""
       self.client.get("/health", name="/health")

   @task(1)  # Rare - list documents
   def list_documents(self):
       """List user's documents."""
       self.client.get(
           "/api/v1/documents/",
           headers=self.headers,
           name="/documents (list)"
       )

4. Add event listener for test summary:
   @events.test_stop.add_listener
   def on_test_stop(environment, **kwargs):
       """Print summary statistics on test completion."""
       stats = environment.stats
       print("\n=== LOAD TEST SUMMARY ===")
       print(f"Total requests: {stats.total.num_requests}")
       print(f"Failures: {stats.total.num_failures}")
       print(f"Median response time: {stats.total.median_response_time}ms")
       print(f"95th percentile: {stats.total.get_response_time_percentile(0.95)}ms")
       print(f"99th percentile: {stats.total.get_response_time_percentile(0.99)}ms")

       # Check if we met the 2s target
       p95 = stats.total.get_response_time_percentile(0.95)
       if p95 and p95 < 2000:
           print("SUCCESS: p95 latency under 2 seconds")
       else:
           print("WARNING: p95 latency exceeds 2 second target")

CRITICAL: Use name parameter in requests for proper grouping in Locust UI.
CRITICAL: Include think time (wait_time) to simulate realistic user behavior.
AVOID: Testing upload endpoint under heavy load (rate limited, resource intensive).
  </action>
  <verify>python -c "from tests.load.locustfile import RAGUser; print('Locustfile loads correctly')"</verify>
  <done>Locust load test file with authenticated user simulation created</done>
</task>

<task type="auto">
  <name>Task 2: Create load testing documentation and runner script</name>
  <files>backend/tests/load/README.md</files>
  <action>
Create backend/tests/load/README.md with:

# Load Testing

## Prerequisites

1. Install locust: `pip install locust`
2. Create a test user in the system:
   - Email: loadtest@example.com
   - Password: loadtestpassword
   (Or set LOAD_TEST_EMAIL and LOAD_TEST_PASSWORD environment variables)

3. Upload some test documents so queries return meaningful results

## Running Load Tests

### Web UI Mode (recommended for exploration)

```bash
cd backend
locust -f tests/load/locustfile.py --host=http://localhost:8000
```

Open http://localhost:8089 in browser, configure:
- Number of users: 100
- Spawn rate: 10 users/second
- Host: http://localhost:8000

### Headless Mode (for CI/scripts)

```bash
cd backend
locust -f tests/load/locustfile.py \
  --host=http://localhost:8000 \
  --users 100 \
  --spawn-rate 10 \
  --run-time 5m \
  --headless \
  --only-summary
```

### Environment Variables

- `LOAD_TEST_EMAIL`: Test user email (default: loadtest@example.com)
- `LOAD_TEST_PASSWORD`: Test user password (default: loadtestpassword)

## Interpreting Results

### Success Criteria

Per Phase 6 requirements:
- p95 latency < 2000ms (2 seconds)
- 100+ concurrent users supported
- Failure rate < 1%

### Key Metrics

| Metric | Target | Description |
|--------|--------|-------------|
| Median | <1000ms | Typical response time |
| p95 | <2000ms | 95th percentile (success criteria) |
| p99 | <3000ms | Worst-case typical |
| RPS | >50 | Requests per second capacity |
| Failures | <1% | Error rate |

### Common Issues

1. **High p99 with normal p50**: Likely circuit breaker activating or cold starts
2. **Increasing latency over time**: Memory leak or connection pool exhaustion
3. **429 errors**: Rate limiting working correctly (expected for high load)
4. **Connection refused**: Server crashed or hit ulimit

## Profiling Bottlenecks

If tests fail targets, profile with:

1. **Database queries**: Check Neo4j/Qdrant query times in traces
2. **OpenAI latency**: Check LLM call duration in metrics
3. **Memory**: Monitor RSS with `docker stats` or similar
  </action>
  <verify>cat backend/tests/load/README.md | head -20</verify>
  <done>Load testing documentation created with clear instructions</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify load test infrastructure</name>
  <what-built>Load testing infrastructure with Locust simulating realistic RAG usage patterns</what-built>
  <how-to-verify>
1. Ensure the app is running: `uvicorn app.main:app --port 8000`
2. Create test user if needed (via /api/v1/auth/register endpoint)
3. Upload at least one test document
4. Run Locust: `cd backend && locust -f tests/load/locustfile.py --host=http://localhost:8000`
5. Open http://localhost:8089
6. Configure: 10 users, 2 users/second spawn rate
7. Run for 1 minute
8. Verify:
   - Requests complete successfully (low failure rate)
   - Response times are reasonable (median < 2s)
   - Stats show breakdown by endpoint (/query, /health, etc.)
  </how-to-verify>
  <resume-signal>Type "load test verified" if tests run correctly, or describe issues encountered</resume-signal>
</task>

</tasks>

<verification>
1. Locustfile validity:
   - python -c "from tests.load.locustfile import RAGUser" succeeds
   - RAGUser has on_start, query_documents, query_enhanced methods

2. Documentation completeness:
   - README explains how to run tests
   - Success criteria clearly stated
   - Troubleshooting section present

3. Test execution (checkpoint):
   - Locust web UI loads
   - Tests execute against running app
   - Statistics collected and displayed
</verification>

<success_criteria>
- Locust load test file created with realistic user scenarios
- Test simulates authentication flow
- Task weights reflect realistic usage (queries >> uploads)
- Documentation explains how to run and interpret results
- Human verified: tests execute successfully against running app
</success_criteria>

<output>
After completion, create `.planning/phases/06-production-hardening/06-04-SUMMARY.md`
</output>
