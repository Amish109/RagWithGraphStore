---
phase: 01-foundation-core-rag
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - backend/app/services/__init__.py
  - backend/app/services/embedding_service.py
  - backend/app/services/generation_service.py
  - backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "Embeddings can be generated for text input"
    - "Embedding dimensions are validated at startup"
    - "LLM generates answers with 'I don't know' fallback"
  artifacts:
    - path: "backend/app/services/embedding_service.py"
      provides: "OpenAI embedding generation with validation"
      exports: ["generate_embeddings", "generate_query_embedding", "validate_embedding_dimensions"]
    - path: "backend/app/services/generation_service.py"
      provides: "LLM answer generation with citation support"
      exports: ["generate_answer"]
  key_links:
    - from: "backend/app/main.py"
      to: "backend/app/services/embedding_service.py"
      via: "startup validation"
      pattern: "validate_embedding_dimensions"
    - from: "backend/app/services/generation_service.py"
      to: "langchain_openai"
      via: "ChatOpenAI import"
      pattern: "ChatOpenAI"
---

<objective>
Implement OpenAI embedding and LLM generation services with startup validation.

Purpose: Provide the AI capabilities required for document indexing (embeddings) and question answering (generation). Embedding dimension validation prevents the critical pitfall of dimension mismatch.
Output: Working embedding service with dimension validation, generation service with "I don't know" fallback.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-core-rag/01-RESEARCH.md
@.planning/phases/01-foundation-core-rag/01-01-SUMMARY.md

Reference research patterns:
- Pattern 7: Document Processing Pipeline (embedding_service.py section)
- Pattern 8: Query Endpoint (generation_service.py section)
- Pitfall 3: Embedding Dimension Mismatch
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OpenAI embedding service with validation</name>
  <files>
    backend/app/services/embedding_service.py
    backend/app/main.py
  </files>
  <action>
    1. Create `backend/app/services/embedding_service.py`:
       - Import AsyncOpenAI from openai
       - Import settings from app.config
       - Import List, asyncio from typing/asyncio

       Initialize client:
       - openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

       async def generate_embeddings(texts: List[str]) -> List[List[float]]:
         - Use openai_client.embeddings.create()
         - model=settings.OPENAI_EMBEDDING_MODEL
         - encoding_format="float"
         - Extract embeddings from response.data
         - Return list of embedding vectors

       async def generate_query_embedding(query: str) -> List[float]:
         - Convenience wrapper for single query
         - Call generate_embeddings([query])
         - Return first embedding

       def validate_embedding_dimensions():
         """Validate embedding dimensions match configuration at startup.

         CRITICAL: Prevents Pitfall #3 - Dimension mismatch causes cryptic errors.
         Call this during FastAPI lifespan startup.
         """
         - Use asyncio.run() for sync context
         - Generate test embedding for "test"
         - Compare len(embedding) with settings.OPENAI_EMBEDDING_DIMENSIONS
         - Raise ValueError with clear message if mismatch:
           f"Embedding dimension mismatch! Expected {expected}, got {actual}. Check OPENAI_EMBEDDING_MODEL and OPENAI_EMBEDDING_DIMENSIONS config."

    2. Update `backend/app/main.py`:
       - Import validate_embedding_dimensions from app.services.embedding_service
       - In lifespan startup, after Qdrant init:
         - Call validate_embedding_dimensions()
         - print("Embedding dimensions validated")
  </action>
  <verify>
    cd backend && python -c "from app.services.embedding_service import generate_embeddings, validate_embedding_dimensions; print('Embedding service imported')"
    cd backend && python -c "import asyncio; from app.services.embedding_service import generate_embeddings; emb = asyncio.run(generate_embeddings(['test'])); print(f'Embedding dim: {len(emb[0])}')"
  </verify>
  <done>
    Embedding service generates vectors using OpenAI API. validate_embedding_dimensions() runs at startup and raises clear error on mismatch. Dimensions match OPENAI_EMBEDDING_DIMENSIONS config.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement LLM generation service with fallback</name>
  <files>
    backend/app/services/generation_service.py
  </files>
  <action>
    1. Create `backend/app/services/generation_service.py`:
       - Import ChatOpenAI from langchain_openai
       - Import ChatPromptTemplate from langchain.prompts
       - Import List, Dict from typing
       - Import settings from app.config

       Initialize LLM:
       - llm = ChatOpenAI(
           model=settings.OPENAI_MODEL,
           temperature=0,  # Deterministic for consistency
           openai_api_key=settings.OPENAI_API_KEY
         )

       async def generate_answer(query: str, context: List[Dict]) -> str:
         """Generate answer using LLM with strict context-only constraint.

         CRITICAL: Prevents hallucination by enforcing "I don't know" fallback.
         Addresses QRY-04 requirement.
         """
         # Assemble context from chunks
         context_text = "\n\n".join([
             f"[Document: {chunk.get('filename', 'Unknown')}]\n{chunk['text']}"
             for chunk in context
         ])

         # Prompt template with strict constraints
         prompt = ChatPromptTemplate.from_messages([
             ("system", """You are a helpful document Q&A assistant. Answer questions ONLY based on the provided context.

CRITICAL INSTRUCTIONS:
- If the context does not contain information to answer the question, respond EXACTLY with: "I don't know. I couldn't find information about this in the provided documents."
- Do not use any knowledge outside the provided context
- Cite the document name when referencing information
- Be concise and direct"""),
             ("user", """Context:
{context}

Question: {query}

Answer:""")
         ])

         # Generate response
         messages = prompt.format_messages(context=context_text, query=query)
         response = await llm.ainvoke(messages)

         return response.content

       async def generate_answer_no_context() -> str:
         """Return standard "I don't know" response when no context available."""
         return "I don't know. I couldn't find any relevant information in your documents."
  </action>
  <verify>
    cd backend && python -c "from app.services.generation_service import generate_answer, llm; print(f'LLM model: {llm.model_name}')"
    cd backend && python -c "import asyncio; from app.services.generation_service import generate_answer_no_context; print(asyncio.run(generate_answer_no_context()))"
  </verify>
  <done>
    Generation service uses ChatOpenAI with temperature=0. generate_answer() includes strict prompt for context-only answers. "I don't know" fallback is implemented for insufficient context.
  </done>
</task>

</tasks>

<verification>
1. Embedding service: generate_embeddings(["test"]) returns vector of correct dimension
2. Dimension validation: validate_embedding_dimensions() completes without error
3. Generation service: llm.model_name matches OPENAI_MODEL setting
4. Generation fallback: generate_answer_no_context() returns "I don't know" message
5. App startup: FastAPI lifespan validates embedding dimensions
</verification>

<success_criteria>
- `asyncio.run(generate_embeddings(["hello world"]))` returns list of 1536-dim vectors
- `validate_embedding_dimensions()` passes without error
- Generation prompt includes "I don't know" instruction
- App starts successfully with embedding validation passing
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-core-rag/01-03-SUMMARY.md`
</output>
