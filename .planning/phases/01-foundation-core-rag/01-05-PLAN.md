---
phase: 01-foundation-core-rag
plan: 05
type: execute
wave: 4
depends_on: ["01-04"]
files_modified:
  - backend/app/services/retrieval_service.py
  - backend/app/db/qdrant_client.py
  - backend/app/models/schemas.py
  - backend/app/api/queries.py
  - backend/app/db/mem0_client.py
  - backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "User can ask natural language questions about documents"
    - "Responses include source citations with document references"
    - "System responds 'I don't know' when context is insufficient"
    - "Mem0 SDK is configured with Neo4j and Qdrant stores"
  artifacts:
    - path: "backend/app/services/retrieval_service.py"
      provides: "Hybrid retrieval with user filtering"
      exports: ["retrieve_relevant_context"]
    - path: "backend/app/api/queries.py"
      provides: "Query endpoint with citations"
      contains: "@router.post"
    - path: "backend/app/db/mem0_client.py"
      provides: "Mem0 initialization (Phase 2 full usage)"
      exports: ["init_mem0", "mem0_memory"]
  key_links:
    - from: "backend/app/api/queries.py"
      to: "backend/app/services/retrieval_service.py"
      via: "context retrieval"
      pattern: "retrieve_relevant_context"
    - from: "backend/app/api/queries.py"
      to: "backend/app/services/generation_service.py"
      via: "answer generation"
      pattern: "generate_answer"
    - from: "backend/app/services/retrieval_service.py"
      to: "backend/app/db/qdrant_client.py"
      via: "vector search"
      pattern: "search_similar_chunks"
---

<objective>
Implement query endpoint with hybrid retrieval, citations, and Mem0 configuration.

Purpose: Complete the RAG loop by enabling users to ask questions and receive answers with source citations. Configure Mem0 for Phase 2 memory integration.
Output: POST /query endpoint returning answers with citations. Mem0 SDK configured and ready.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-core-rag/01-RESEARCH.md
@.planning/phases/01-foundation-core-rag/01-01-SUMMARY.md
@.planning/phases/01-foundation-core-rag/01-02-SUMMARY.md
@.planning/phases/01-foundation-core-rag/01-03-SUMMARY.md
@.planning/phases/01-foundation-core-rag/01-04-SUMMARY.md

Reference research patterns:
- Pattern 8: Query Endpoint with Hybrid Retrieval
- Code Examples: Mem0 Configuration with Neo4j + Qdrant
- Pitfall 6: No Multi-Tenant Filtering (CRITICAL)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement hybrid retrieval with user filtering</name>
  <files>
    backend/app/db/qdrant_client.py
    backend/app/services/retrieval_service.py
  </files>
  <action>
    1. Update `backend/app/db/qdrant_client.py` to add search function:
       def search_similar_chunks(query_vector: List[float], user_id: str, limit: int = 10) -> List[Dict]:
         """Search for similar chunks filtered by user_id.

         CRITICAL: Always filter by user_id for multi-tenant isolation.
         """
         - Import Filter, FieldCondition, MatchValue from qdrant_client.models
         - Use qdrant_client.search() with:
           - query_filter=Filter(must=[FieldCondition(key="user_id", match=MatchValue(value=user_id))])
           - with_payload=True
           - with_vectors=False (save bandwidth)
         - Return list of {id, score, text, document_id, position}

    2. Create `backend/app/services/retrieval_service.py`:
       - Import generate_query_embedding from embedding_service
       - Import search_similar_chunks from qdrant_client
       - Import neo4j_driver from neo4j_client
       - Import settings from config
       - Import List, Dict from typing

       async def retrieve_relevant_context(query: str, user_id: str, max_results: int = 3) -> Dict:
         """Retrieve relevant context using vector search.

         Phase 1: Vector-only retrieval from Qdrant.
         Phase 2+: Add graph enrichment with Neo4j.

         CRITICAL: Always filter by user_id for multi-tenant isolation.
         """
         # Step 1: Generate query embedding
         query_embedding = await generate_query_embedding(query)

         # Step 2: Vector search in Qdrant (filtered by user_id)
         similar_chunks = search_similar_chunks(
             query_vector=query_embedding,
             user_id=user_id,
             limit=max_results
         )

         # Step 3: Enrich with document metadata from Neo4j
         enriched_chunks = []
         with neo4j_driver.session(database=settings.NEO4J_DATABASE) as session:
             for chunk in similar_chunks:
                 result = session.run("""
                     MATCH (c:Chunk {id: $chunk_id})<-[:CONTAINS]-(d:Document)
                     RETURN d.filename AS filename, d.id AS document_id
                 """, chunk_id=chunk["id"])

                 record = result.single()
                 if record:
                     enriched_chunks.append({
                         **chunk,
                         "filename": record["filename"],
                         "document_id": record["document_id"]
                     })

         return {"chunks": enriched_chunks}
  </action>
  <verify>
    cd backend && python -c "from app.db.qdrant_client import search_similar_chunks; print('Qdrant search imported')"
    cd backend && python -c "from app.services.retrieval_service import retrieve_relevant_context; print('Retrieval service imported')"
  </verify>
  <done>
    Vector search filters by user_id for multi-tenant isolation. Document metadata is enriched from Neo4j. Retrieval returns chunks with text, score, filename, and document_id.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create query endpoint with citations</name>
  <files>
    backend/app/models/schemas.py
    backend/app/api/queries.py
    backend/app/main.py
  </files>
  <action>
    1. Update `backend/app/models/schemas.py`:
       class QueryRequest(BaseModel):
           query: str
           max_results: int = 3

       class Citation(BaseModel):
           document_id: str
           filename: str
           chunk_text: str
           relevance_score: float

       class QueryResponse(BaseModel):
           answer: str
           citations: List[Citation]

    2. Create `backend/app/api/queries.py`:
       - Import APIRouter, Depends, HTTPException from fastapi
       - Import get_current_user from app.core.security
       - Import retrieve_relevant_context from app.services.retrieval_service
       - Import generate_answer, generate_answer_no_context from app.services.generation_service
       - Import QueryRequest, QueryResponse, Citation from app.models.schemas

       router = APIRouter()

       @router.post("/", response_model=QueryResponse)
       async def query_documents(
           request: QueryRequest,
           current_user: dict = Depends(get_current_user)
       ):
         """Ask a question about uploaded documents (QRY-01, QRY-03, QRY-04)."""
         user_id = current_user["id"]

         # Step 1: Retrieve relevant context
         context = await retrieve_relevant_context(
             query=request.query,
             user_id=user_id,
             max_results=request.max_results
         )

         # Step 2: Handle no context case (QRY-04)
         if not context["chunks"]:
             return QueryResponse(
                 answer=await generate_answer_no_context(),
                 citations=[]
             )

         # Step 3: Generate answer
         answer = await generate_answer(
             query=request.query,
             context=context["chunks"]
         )

         # Step 4: Format citations (QRY-03)
         citations = [
             Citation(
                 document_id=chunk["document_id"],
                 filename=chunk["filename"],
                 chunk_text=chunk["text"][:200] + "..." if len(chunk["text"]) > 200 else chunk["text"],
                 relevance_score=chunk["score"]
             )
             for chunk in context["chunks"]
         ]

         return QueryResponse(answer=answer, citations=citations)

    3. Update `backend/app/main.py`:
       - Import queries router: from app.api.queries import router as queries_router
       - Include router: app.include_router(queries_router, prefix=f"{settings.API_V1_PREFIX}/query", tags=["queries"])
  </action>
  <verify>
    cd backend && python -c "from app.api.queries import router; print(f'Query routes: {[r.path for r in router.routes]}')"
    cd backend && python -c "from app.main import app; print([r.path for r in app.routes if 'query' in r.path])"
  </verify>
  <done>
    Query endpoint accepts natural language questions. Response includes answer with source citations. "I don't know" fallback works when no relevant context found.
  </done>
</task>

<task type="auto">
  <name>Task 3: Configure Mem0 SDK for Phase 2</name>
  <files>
    backend/app/db/mem0_client.py
  </files>
  <action>
    1. Create `backend/app/db/mem0_client.py` following research code example:
       - Import Memory from mem0
       - Import settings from app.config
       - Import Optional from typing

       def init_mem0() -> Memory:
         """Initialize Mem0 with dual stores (Neo4j + Qdrant).

         Phase 1: Basic configuration only. Full integration in Phase 2.
         NOTE: Uses separate "memory" collection from documents to prevent
         confusion between RAG documents and user memory (Pitfall #1).
         """
         config = {
             "version": "v1.1",
             "llm": {
                 "provider": "openai",
                 "config": {
                     "model": settings.OPENAI_MODEL,
                     "temperature": 0.1,
                 }
             },
             "embedder": {
                 "provider": "openai",
                 "config": {
                     "model": settings.OPENAI_EMBEDDING_MODEL,
                 }
             },
             "vector_store": {
                 "provider": "qdrant",
                 "config": {
                     "collection_name": "memory",  # SEPARATE from documents collection
                     "host": settings.QDRANT_HOST,
                     "port": settings.QDRANT_PORT,
                 }
             },
             "graph_store": {
                 "provider": "neo4j",
                 "config": {
                     "url": settings.NEO4J_URI,
                     "username": settings.NEO4J_USERNAME,
                     "password": settings.NEO4J_PASSWORD,
                 }
             },
         }

         # Add API key for Qdrant Cloud if configured
         if settings.QDRANT_API_KEY:
             config["vector_store"]["config"]["api_key"] = settings.QDRANT_API_KEY

         memory = Memory.from_config(config)
         return memory

       # Lazy initialization (will be used in Phase 2)
       _mem0_memory: Optional[Memory] = None

       def get_mem0() -> Memory:
         """Get or initialize Mem0 memory instance."""
         global _mem0_memory
         if _mem0_memory is None:
             _mem0_memory = init_mem0()
         return _mem0_memory

    NOTE: This configures Mem0 but doesn't actively use it. Full memory integration (user preferences, conversation history) comes in Phase 2 per research recommendation to keep Phase 1 focused on core RAG.
  </action>
  <verify>
    cd backend && python -c "from app.db.mem0_client import init_mem0; print('Mem0 client imported')"
    cd backend && python -c "from app.db.mem0_client import get_mem0; print('get_mem0 function available')"
  </verify>
  <done>
    Mem0 SDK configured with Neo4j graph store and Qdrant vector store. Uses separate "memory" collection from documents. Ready for Phase 2 memory integration.
  </done>
</task>

</tasks>

<verification>
1. Retrieval: retrieve_relevant_context() returns chunks with metadata
2. Query endpoint: POST /api/v1/query returns answer with citations
3. No context: Query with no matching documents returns "I don't know"
4. Citations: Response includes document_id, filename, chunk_text, relevance_score
5. Auth: Query without token returns 401
6. Mem0: init_mem0() creates valid Memory instance
7. FastAPI docs: /docs shows query endpoint with request/response models
</verification>

<success_criteria>
- `curl -X POST /api/v1/query -H "Authorization: Bearer $TOKEN" -d '{"query":"What is X?"}' -H "Content-Type: application/json"` returns {"answer": "...", "citations": [...]}
- Query about non-existent topic returns "I don't know" message
- Citations include filename and truncated chunk text
- Mem0 initializes without error (even if not actively used yet)
- All Phase 1 success criteria are met:
  1. User can register and login with email/password
  2. User can upload PDF and DOCX documents
  3. User can ask questions and receive answers with citations
  4. System responds "I don't know" when context insufficient
  5. All configuration via environment variables
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-core-rag/01-05-SUMMARY.md`
</output>
