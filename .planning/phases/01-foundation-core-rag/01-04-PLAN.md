---
phase: 01-foundation-core-rag
plan: 04
type: execute
wave: 3
depends_on: ["01-02", "01-03"]
files_modified:
  - backend/app/services/document_processor.py
  - backend/app/services/indexing_service.py
  - backend/app/models/document.py
  - backend/app/models/schemas.py
  - backend/app/api/documents.py
  - backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "User can upload PDF documents that are parsed and stored"
    - "User can upload DOCX documents that are parsed and stored"
    - "Documents are chunked using semantic splitting"
    - "Chunks are indexed in both Neo4j and Qdrant with shared IDs"
    - "Processing happens in background without blocking API"
  artifacts:
    - path: "backend/app/services/document_processor.py"
      provides: "PDF/DOCX parsing and semantic chunking"
      exports: ["extract_text_from_pdf", "extract_text_from_docx", "chunk_text", "process_document_pipeline"]
    - path: "backend/app/services/indexing_service.py"
      provides: "Dual-write to Neo4j and Qdrant"
      exports: ["store_document_in_neo4j", "store_chunks_in_qdrant"]
    - path: "backend/app/api/documents.py"
      provides: "Document upload endpoint"
      contains: "@router.post(\"/upload\")"
  key_links:
    - from: "backend/app/api/documents.py"
      to: "backend/app/services/document_processor.py"
      via: "background task"
      pattern: "process_document_pipeline"
    - from: "backend/app/services/document_processor.py"
      to: "backend/app/services/embedding_service.py"
      via: "embedding generation"
      pattern: "generate_embeddings"
    - from: "backend/app/services/indexing_service.py"
      to: "backend/app/db/neo4j_client.py"
      via: "Neo4j storage"
      pattern: "neo4j_driver.session"
    - from: "backend/app/services/indexing_service.py"
      to: "backend/app/db/qdrant_client.py"
      via: "Qdrant storage"
      pattern: "upsert_chunks"
---

<objective>
Implement complete document upload and processing pipeline with dual-store indexing.

Purpose: Enable users to upload PDF and DOCX documents that are parsed, chunked semantically, embedded, and indexed in both Neo4j (metadata/relationships) and Qdrant (vectors). This is the core of the RAG system.
Output: POST /documents/upload endpoint with background processing, dual-write to Neo4j and Qdrant.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-core-rag/01-RESEARCH.md
@.planning/phases/01-foundation-core-rag/01-01-SUMMARY.md
@.planning/phases/01-foundation-core-rag/01-02-SUMMARY.md
@.planning/phases/01-foundation-core-rag/01-03-SUMMARY.md

Reference research patterns:
- Pattern 6: Document Upload with Async Processing
- Pattern 7: Document Processing Pipeline
- Pitfall 2: Poor Chunking Strategy (CRITICAL)
- Pitfall 6: No Multi-Tenant Filtering (CRITICAL)
- Pitfall 7: Synchronous Document Processing
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement PDF/DOCX parsing with semantic chunking</name>
  <files>
    backend/app/services/document_processor.py
  </files>
  <action>
    1. Create `backend/app/services/document_processor.py`:
       - Import pymupdf4llm for PDF
       - Import Document from docx for DOCX
       - Import RecursiveCharacterTextSplitter from langchain.text_splitter
       - Import settings from app.config
       - Import List, Dict from typing
       - Import os, uuid

       def extract_text_from_pdf(file_path: str) -> str:
         """Extract text from PDF using pymupdf4llm.

         Returns clean Markdown for semantic chunking.
         """
         - md_text = pymupdf4llm.to_markdown(file_path)
         - return md_text

       def extract_text_from_docx(file_path: str) -> str:
         """Extract text from DOCX using python-docx."""
         - doc = Document(file_path)
         - Extract paragraphs and tables
         - Join with double newlines for semantic boundaries
         - return text

       def chunk_text(text: str) -> List[Dict]:
         """Chunk text using semantic chunking.

         CRITICAL: Use RecursiveCharacterTextSplitter, NOT fixed-size.
         Prevents Pitfall #2 (poor chunking strategy).
         """
         - splitter = RecursiveCharacterTextSplitter(
             chunk_size=settings.CHUNK_SIZE,
             chunk_overlap=settings.CHUNK_OVERLAP,
             length_function=len,
             separators=["\n\n", "\n", ". ", " ", ""]  # Respect semantic boundaries
           )
         - chunks = splitter.split_text(text)
         - Return [{"text": chunk, "position": idx} for idx, chunk in enumerate(chunks)]

       async def process_document_pipeline(
           file_path: str,
           document_id: str,
           user_id: str,
           filename: str
       ):
         """Complete document processing pipeline.

         Steps:
         1. Extract text (PDF/DOCX)
         2. Chunk text (semantic chunking)
         3. Generate embeddings
         4. Store in Neo4j (metadata, chunks)
         5. Store in Qdrant (vectors)
         6. Clean up temp file

         Runs in background task to avoid blocking API.
         """
         - Import generate_embeddings from embedding_service
         - Import store_document_in_neo4j, store_chunks_in_qdrant from indexing_service

         try:
           - Detect file type by extension
           - Extract text (pdf or docx)
           - Chunk text
           - Generate embeddings for all chunks
           - Prepare chunk_data with shared UUIDs for Neo4j/Qdrant linkage:
             [{"id": uuid4(), "text": ..., "position": ..., "vector": ..., "document_id": ..., "user_id": ...}]
           - Store in Neo4j
           - Store in Qdrant
           - Log success
         except Exception as e:
           - Log error
           - Raise (TODO: store error status for user notification)
         finally:
           - Clean up temp file: os.unlink(file_path) if exists
  </action>
  <verify>
    cd backend && python -c "from app.services.document_processor import chunk_text; chunks = chunk_text('Hello world. This is a test.'); print(f'Chunks: {len(chunks)}')"
    cd backend && python -c "from app.services.document_processor import extract_text_from_pdf, extract_text_from_docx; print('Extractors imported')"
  </verify>
  <done>
    PDF extraction uses pymupdf4llm for clean Markdown. DOCX extraction handles paragraphs and tables. Semantic chunking uses RecursiveCharacterTextSplitter with proper separators. Pipeline is async-ready for background execution.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement dual-write indexing and upload endpoint</name>
  <files>
    backend/app/services/indexing_service.py
    backend/app/db/qdrant_client.py
    backend/app/models/document.py
    backend/app/models/schemas.py
    backend/app/api/documents.py
    backend/app/main.py
  </files>
  <action>
    1. Update `backend/app/db/qdrant_client.py` to add upsert function:
       def upsert_chunks(chunks: List[Dict]) -> None:
         """Insert or update document chunks with embeddings.

         Args: chunks with keys: id, vector, text, document_id, user_id, position, metadata
         CRITICAL: Always include user_id in payload for multi-tenant filtering.
         """
         - Import PointStruct from qdrant_client.models
         - Create PointStruct for each chunk
         - Use qdrant_client.upsert()

    2. Create `backend/app/services/indexing_service.py`:
       - Import neo4j_driver from app.db.neo4j_client
       - Import upsert_chunks from app.db.qdrant_client
       - Import settings from app.config
       - Import List, Dict from typing

       def store_document_in_neo4j(document_id: str, user_id: str, filename: str, chunks: List[Dict]):
         """Store document metadata and chunks in Neo4j with relationships."""
         with neo4j_driver.session(database=settings.NEO4J_DATABASE) as session:
           # Create Document node with OWNS relationship to User
           - MATCH (u:User {id: $user_id})
           - CREATE (d:Document {id, filename, user_id, upload_date: datetime(), chunk_count})
           - CREATE (u)-[:OWNS]->(d)

           # Create Chunk nodes with CONTAINS relationship to Document
           - For each chunk:
             - MATCH (d:Document {id: $document_id})
             - CREATE (c:Chunk {id, document_id, text, position, embedding_id})
             - CREATE (d)-[:CONTAINS]->(c)

       def store_chunks_in_qdrant(chunks: List[Dict]):
         """Store chunk embeddings in Qdrant."""
         - Call upsert_chunks(chunks)

    3. Create `backend/app/models/document.py`:
       - get_document_by_id(document_id: str, user_id: str) -> Optional[dict]
         - CRITICAL: Always filter by user_id for multi-tenant isolation
       - get_user_documents(user_id: str) -> List[dict]
         - Return all documents for a user

    4. Update `backend/app/models/schemas.py`:
       - DocumentUploadResponse(BaseModel): document_id, filename, status, message
       - DocumentInfo(BaseModel): id, filename, upload_date, chunk_count

    5. Create `backend/app/api/documents.py` following research Pattern 6:
       - Import APIRouter, UploadFile, File, Depends, HTTPException, BackgroundTasks
       - Import get_current_user from app.core.security
       - Import process_document_pipeline from app.services.document_processor

       router = APIRouter()

       @router.post("/upload", response_model=DocumentUploadResponse)
       async def upload_document(
           file: UploadFile = File(...),
           background_tasks: BackgroundTasks,
           current_user: dict = Depends(get_current_user)
       ):
         """Upload PDF or DOCX document (API-01, DOC-01, DOC-02)."""
         - Validate content_type: application/pdf or application/vnd.openxmlformats-officedocument.wordprocessingml.document
         - Generate document_id with uuid4
         - Get user_id from current_user["id"]
         - Save file to temp location
         - Add process_document_pipeline to background_tasks
         - Return DocumentUploadResponse with status="processing"

    6. Update `backend/app/main.py`:
       - Import documents router: from app.api.documents import router as documents_router
       - Include router: app.include_router(documents_router, prefix=f"{settings.API_V1_PREFIX}/documents", tags=["documents"])
  </action>
  <verify>
    cd backend && python -c "from app.services.indexing_service import store_document_in_neo4j, store_chunks_in_qdrant; print('Indexing service imported')"
    cd backend && python -c "from app.api.documents import router; print(f'Document routes: {[r.path for r in router.routes]}')"
    cd backend && python -c "from app.main import app; print([r.path for r in app.routes if 'documents' in r.path])"
  </verify>
  <done>
    Upload endpoint accepts PDF/DOCX with size validation. Background task processes documents without blocking. Neo4j stores document and chunk nodes with relationships. Qdrant stores embeddings with user_id for multi-tenant filtering. Shared UUIDs link Neo4j chunks to Qdrant vectors.
  </done>
</task>

</tasks>

<verification>
1. PDF parsing: extract_text_from_pdf() returns text from test PDF
2. DOCX parsing: extract_text_from_docx() returns text from test DOCX
3. Chunking: chunk_text() uses semantic separators and respects size limits
4. Upload endpoint: POST /api/v1/documents/upload returns 200 with document_id
5. Authentication: Upload without token returns 401
6. File validation: Non-PDF/DOCX upload returns 400
7. Background processing: Document appears in Neo4j and Qdrant after processing
</verification>

<success_criteria>
- `curl -X POST /api/v1/documents/upload -H "Authorization: Bearer $TOKEN" -F "file=@test.pdf"` returns {"document_id": "...", "status": "processing"}
- After processing, Neo4j has Document node with OWNS relationship to User
- After processing, Neo4j has Chunk nodes with CONTAINS relationships to Document
- After processing, Qdrant has vectors with user_id in payload
- Chunks in Neo4j have embedding_id matching Qdrant point IDs
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-core-rag/01-04-SUMMARY.md`
</output>
