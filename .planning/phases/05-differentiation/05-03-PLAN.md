---
phase: 05-differentiation
plan: 03
type: execute
wave: 2
depends_on: []
files_modified:
  - backend/app/services/confidence_service.py
  - backend/app/services/generation_service.py
  - backend/app/services/retrieval_service.py
  - backend/app/api/queries.py
  - backend/app/models/schemas.py
autonomous: true

must_haves:
  truths:
    - "Query responses include confidence scores indicating model certainty"
    - "Confidence scores include interpretation (high/medium/low)"
    - "Citations include highlighted passages showing exact text"
    - "Highlighted passages are verbatim from source (not paraphrased)"
    - "Character offsets enable UI highlighting"
  artifacts:
    - path: "backend/app/services/confidence_service.py"
      provides: "Confidence calculation from OpenAI logprobs"
      exports: ["calculate_confidence_from_logprobs", "generate_answer_with_confidence"]
    - path: "backend/app/models/schemas.py"
      provides: "HighlightedCitation, ConfidenceScore, QueryResponseWithCitations schemas"
      contains: "class HighlightedCitation"
    - path: "backend/app/services/retrieval_service.py"
      provides: "Citation extraction with exact text passages"
      contains: "extract_highlighted_citations"
  key_links:
    - from: "backend/app/api/queries.py"
      to: "backend/app/services/confidence_service.py"
      via: "import generate_answer_with_confidence"
      pattern: "from app.services.confidence_service import"
    - from: "backend/app/api/queries.py"
      to: "backend/app/services/retrieval_service.py"
      via: "import extract_highlighted_citations"
      pattern: "extract_highlighted_citations"
---

<objective>
Implement confidence scores and highlighted citations for query responses.

Purpose: Build user trust by showing confidence scores (Success Criteria #6) and enabling verification through exact text passage citations (Success Criteria #4). Users can see exactly where answers come from and know when to verify.

Output: Confidence service with logprobs, enhanced citation extraction, updated schemas, and enhanced query endpoint.
</objective>

<execution_context>
@/Users/apple/.claude/get-shit-done/workflows/execute-plan.md
@/Users/apple/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-differentiation/05-RESEARCH.md

# Existing files to extend
@backend/app/services/generation_service.py
@backend/app/services/retrieval_service.py
@backend/app/api/queries.py
@backend/app/models/schemas.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Confidence Service</name>
  <files>backend/app/services/confidence_service.py</files>
  <action>
Create new confidence service for calculating response confidence from OpenAI logprobs:

1. Implement `calculate_confidence_from_logprobs(logprobs: List[Dict]) -> dict`:
   - Handle empty/null logprobs gracefully (return score=0.5, level="unknown")
   - Extract log probabilities from list
   - Convert to linear probabilities using np.exp(logprob)
   - Calculate geometric mean (most stable for sequences): np.exp(np.mean(log_probs))
   - Calculate perplexity: np.exp(-np.mean(log_probs))
   - Determine confidence level:
     - score >= 0.85: "high" + "The model is highly confident in this response."
     - score >= 0.60: "medium" + "The model is moderately confident. Consider verifying key claims."
     - score < 0.60: "low" + "The model has low confidence. Please verify this response with authoritative sources."
   - Return dict with: score (rounded to 3 decimals), level, interpretation, metrics (avg_probability, geometric_mean, perplexity, tokens_analyzed)

2. Implement `generate_answer_with_confidence(query: str, context: List[Dict]) -> dict`:
   - Create ChatOpenAI instance with logprobs=True enabled
   - Use existing prompt pattern from generation_service.py
   - Invoke LLM and get response
   - Extract logprobs from response.response_metadata['logprobs']['content']
   - Call calculate_confidence_from_logprobs
   - Return dict with: answer, confidence

Import numpy as np for math operations.
Use ChatOpenAI from langchain_openai.
Use langchain_core.prompts.ChatPromptTemplate.
  </action>
  <verify>
```bash
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "from app.services.confidence_service import calculate_confidence_from_logprobs, generate_answer_with_confidence; print('Imports OK')"
```
  </verify>
  <done>Confidence service exists with calculate_confidence_from_logprobs and generate_answer_with_confidence. Logprobs-based confidence calculation implemented.</done>
</task>

<task type="auto">
  <name>Task 2: Add Enhanced Citation Extraction</name>
  <files>backend/app/services/retrieval_service.py, backend/app/models/schemas.py</files>
  <action>
1. In schemas.py, add enhanced citation and response schemas:

```python
class ConfidenceScore(BaseModel):
    """Schema for confidence score with interpretation."""
    score: float  # 0.0 to 1.0
    level: str  # "high", "medium", "low", "unknown"
    interpretation: str

class HighlightedCitation(BaseModel):
    """Schema for citation with exact text passage highlighting."""
    document_id: str
    filename: str
    page_number: Optional[int] = None
    chunk_text: str  # Full chunk for context
    highlighted_passage: str  # Exact text supporting answer
    highlight_start: int  # Character offset in chunk_text
    highlight_end: int  # Character offset in chunk_text
    relevance_score: float
    chunk_position: int

class QueryResponseWithCitations(BaseModel):
    """Schema for enhanced query response with confidence and highlighted citations."""
    answer: str
    confidence: ConfidenceScore
    citations: List[HighlightedCitation]
```

2. In retrieval_service.py, add `extract_highlighted_citations` function:
```python
async def extract_highlighted_citations(
    answer: str,
    context_chunks: List[Dict],
    query: str
) -> List[HighlightedCitation]:
```

Implementation:
- For each context chunk, use LLM to identify the exact passage supporting the answer
- Prompt: Ask LLM to return JSON with highlighted_passage (verbatim from chunk)
- CRITICAL: Verify highlighted_passage exists in chunk_text using string search
- Calculate highlight_start and highlight_end as character offsets
- If exact match fails (start < 0), fallback to first 200 chars of chunk
- Return list of HighlightedCitation objects

Use json.loads to parse LLM response. Handle parse errors with fallback.
Import HTTPException (not needed here), json, and schemas from app.models.schemas.
  </action>
  <verify>
```bash
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "from app.services.retrieval_service import extract_highlighted_citations; from app.models.schemas import HighlightedCitation, ConfidenceScore, QueryResponseWithCitations; print('Imports OK')"
```
  </verify>
  <done>HighlightedCitation, ConfidenceScore, QueryResponseWithCitations schemas added. extract_highlighted_citations function added to retrieval_service.py.</done>
</task>

<task type="auto">
  <name>Task 3: Add Enhanced Query Endpoint</name>
  <files>backend/app/api/queries.py</files>
  <action>
Add new endpoint for enhanced query with confidence and highlighted citations:

```python
@router.post("/enhanced", response_model=QueryResponseWithCitations)
async def query_documents_enhanced(
    request: QueryRequest,
    current_user: dict = Depends(get_current_user),
):
```

Implementation:
1. Retrieve relevant context (same as existing query endpoint)
2. Handle no context case - return with high confidence "I don't know"
3. Generate answer with confidence using generate_answer_with_confidence
4. Extract highlighted citations using extract_highlighted_citations
5. Return QueryResponseWithCitations

Import from confidence_service: generate_answer_with_confidence
Import from retrieval_service: extract_highlighted_citations
Import from schemas: QueryResponseWithCitations, ConfidenceScore, HighlightedCitation

The existing /query endpoint remains unchanged for backward compatibility.
The new /enhanced endpoint provides Phase 5 features.
  </action>
  <verify>
```bash
cd /Users/apple/Desktop/RAGWithGraphStore/backend
python -c "from app.api.queries import router; print('Router has routes:', [r.path for r in router.routes])"
```
  </verify>
  <done>POST /enhanced endpoint added to queries.py. Returns QueryResponseWithCitations with confidence scores and highlighted citations.</done>
</task>

</tasks>

<verification>
```bash
cd /Users/apple/Desktop/RAGWithGraphStore/backend

# Verify all imports work
python -c "
from app.services.confidence_service import calculate_confidence_from_logprobs, generate_answer_with_confidence
from app.services.retrieval_service import extract_highlighted_citations
from app.models.schemas import HighlightedCitation, ConfidenceScore, QueryResponseWithCitations
print('All imports successful')
"

# Verify confidence thresholds are correct
python -c "
from app.services.confidence_service import calculate_confidence_from_logprobs
import math
# Test with mock logprobs (high confidence)
high_conf = calculate_confidence_from_logprobs([{'logprob': math.log(0.9)} for _ in range(10)])
print(f'High conf test: score={high_conf[\"score\"]}, level={high_conf[\"level\"]}')
# Test with low confidence
low_conf = calculate_confidence_from_logprobs([{'logprob': math.log(0.3)} for _ in range(10)])
print(f'Low conf test: score={low_conf[\"score\"]}, level={low_conf[\"level\"]}')
"

# Verify no syntax errors
python -m py_compile app/services/confidence_service.py
python -m py_compile app/services/retrieval_service.py
python -m py_compile app/api/queries.py
python -m py_compile app/models/schemas.py
echo "Syntax validation passed"
```
</verification>

<success_criteria>
1. confidence_service.py exists with calculate_confidence_from_logprobs, generate_answer_with_confidence
2. Confidence thresholds: >=0.85 high, >=0.60 medium, <0.60 low
3. HighlightedCitation schema includes: highlighted_passage, highlight_start, highlight_end
4. ConfidenceScore schema includes: score, level, interpretation
5. extract_highlighted_citations verifies passage exists in chunk (no hallucination)
6. POST /enhanced endpoint returns QueryResponseWithCitations
7. All files pass syntax validation
8. Existing /query endpoint unchanged (backward compatible)
</success_criteria>

<output>
After completion, create `.planning/phases/05-differentiation/05-03-SUMMARY.md`
</output>
